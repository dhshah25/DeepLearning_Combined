{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W0yMH9JUy4rQ",
    "outputId": "d422b6bd-36e4-4f8e-9876-04b216665189"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Cl2e0io599C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6ctpoEF5_QH"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OnPXBTk26Bvv",
    "outputId": "7079a096-c337-4267-f362-9f46275fd124"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yP3-Q-mk6FPc"
   },
   "source": [
    "**Loading**\n",
    "\n",
    "- Load the Enron spam dataset from Hugging Face. The dataset has a \"train\" and \"test\" split.\n",
    "- We have further splitted the training set into training and validation sets (90/10 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418,
     "referenced_widgets": [
      "58ce045a309548ccb1f9f3604ef3f412",
      "33e6f971eef4411a90cd257aa4fc2f3c",
      "bfa67237e59249dbbef6426ebf249ed7",
      "fd7e2382717d4d3392813f1e6044f939",
      "bf8ee6d25ed1474ab053cd54a52096f8",
      "d67aec7c972f4b3fb5cf14b2729ae423",
      "e3e8b638f6ee4f14afc69849d5fb1289",
      "613212a8875c4856804b8e1841689ab7",
      "129131a61b4f48688003621d084ebc5e",
      "b7a66fca70e848e1b8b356c3974f54d7",
      "4256a75069494e068c9ed8d33a93cd56",
      "0a8bc68931614797ab56d64816f1fdf4",
      "dfaeaf05b0274c378aecf832f6ddd5c3",
      "92eb7afdabf447bc9f6310889d8d747d",
      "d9b3ae1e8c4d415a929f755df03d652b",
      "72e003a3a7f340d388386ee680d4ac19",
      "59e64361eca6438ca0831cefa81a0cca",
      "252d4e8d3de64bfba2fbb295f0b27f3d",
      "30c803e5e83442f79e6a85173ddfd464",
      "220a5ca9df944878af97e58890e4003d",
      "4df07dff786b479e873065880dbfa776",
      "67463d7ebe654001b6a25d41c4ea2b49",
      "85813159548940bf98c9080acd1b6237",
      "7bc1f6e35d6d413e8bec937dcf48b6a1",
      "636e39246f4e492db14136b471c14034",
      "c36706bf7fb44f7abf3e80d667f968ea",
      "4e98ccf95f2b4dad9c71bcb6078bbd03",
      "aeef1328f22442f68e1936aa3af167a8",
      "9e684d02976a40c99a2fa3b0328d6031",
      "76b5f3d2005c4656930151a8af08164c",
      "8b221df346a14bf9bf65ed956a953162",
      "4c640d3de14d4df3879c5b5b1b0e8b60",
      "e83b03abfd2641299ff5d21e51cdbb41",
      "c3990e56d37743e280466cc47a772744",
      "5942af82a9a242e09bb7808793f5ccb5",
      "cafb316eb6ff4f79982bc68c6c554ba6",
      "754f2a4bb008491bac505092fafc3768",
      "f4d3797508d34066938735a78d87b2bf",
      "cc4dd45551bd4ba7b6f24305b3fa168a",
      "f9cc9903c53942a59d1fac19df462032",
      "ade644c4ca12463a8259e2bddd5efc8a",
      "49e5485485e84769a2769470b12fa5a7",
      "0fc3eec08d484dddb225110b68bd4513",
      "c526aba59879470f9abad4913912439a",
      "0504b86dc3a04384a75653956dced2c1",
      "a2ac41e8fe8949ab8b84dcd9e1fb7189",
      "6de180ff0a144d41aff356f1c67ef2af",
      "b586019612a945fd8c2611977a4a65a7",
      "1dfc5118289e435d8b5271ba00f4b875",
      "959b2229855343b891a30c5f7caf0c56",
      "56ad2ac844fb4b9d870afd052f6a2a15",
      "f123cca8d7a44bfe9a4049ca74311cec",
      "f7d9daf092e04d00b184d277b109b444",
      "e4f75542c7414d038b81efdc12709b89",
      "1e7cf738098f4344bdf2ff9fad100d27"
     ]
    },
    "id": "mW2r0kgf6Nfk",
    "outputId": "9f16da73-1ba1-48e9-a895-b81c4b638866"
   },
   "outputs": [],
   "source": [
    "raw_train = load_dataset(\"SetFit/enron_spam\", split=\"train\")\n",
    "raw_test = load_dataset(\"SetFit/enron_spam\", split=\"test\")\n",
    "\n",
    "raw_train_val = raw_train.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = raw_train_val['train']\n",
    "val_dataset = raw_train_val['test']\n",
    "test_dataset = raw_test\n",
    "\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Validation samples:\", len(val_dataset))\n",
    "print(\"Test samples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwg0wk6e6TdV"
   },
   "source": [
    "**Tokenization**\n",
    "\n",
    "- We define a tokenization function that uses the pre-trained model’s tokenizer. Here, we use a maximum sequence length of 128 tokens. The tokenize_function does truncation and padding so that each sequence has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e7poNg3V6bXC"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples, tokenizer, max_length=128):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4FHDdSQj6dlA"
   },
   "source": [
    "**Data Collation**\n",
    "\n",
    "- This function is using torch.stack() so that batches are correctly created without causing type conversion issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-0cxJmH6idf"
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['label'] for item in batch])\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ro0dvo26lGw"
   },
   "source": [
    "**Custom Classifier Model Design**\n",
    "\n",
    "- **Architecture:**  \n",
    "  1. **Encoder:**  \n",
    "     We hace loaded pre-trained encoder (either DistilBERT or TinyBERT) using AutoModel.from_pretrained(). All encoder weights are frozen to focus on utilizing the encoder’s feature extraction capability without updating its parameters.\n",
    "  2. **Classification Head (MLP):**  \n",
    "     We have added an MLP on top of the encoder. It has a linear layer that will reduce the dimensionality, a ReLU activation function, a Dropout for regularization and a final linear layer that produces logits for binary classification.\n",
    "- **Representation Extraction:**  \n",
    "  This model uses the embedding of the CLS token from the encoder's output as the representation of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3-jcUG06uiZ"
   },
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, hidden_size, dropout_rate=0.1):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(pretrained_model_name)\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size // 2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = outputs.last_hidden_state\n",
    "        cls_embedding = hidden_state[:, 0, :]\n",
    "        logits = self.classifier(cls_embedding)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fs8TFJ-J62X-"
   },
   "source": [
    "**Training and Evaluation Process**\n",
    "\n",
    "- **Training Loop:** We have used CrossEntropyLoss as it is suitable for classification tasks. The Adam optimizer is applied to update only the classifier head weights and to keep the encoder weights frozen. During training, the model’s performance can be tracked via accuracy, precision, recall, and F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iTLKSR1d66GQ"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=3, learning_rate=1e-3):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_metrics = evaluate_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f} | Val Acc: {val_metrics['accuracy']:.4f}, \"\n",
    "              f\"Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation:**  \n",
    "  - We are checking the performance on the validation set at each epoch and the test set is used for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2Yeyx-p6-Uk"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    preds, labels_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            batch_preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            preds.extend(batch_preds.cpu().numpy())\n",
    "            labels_all.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(labels_all, preds)\n",
    "    precision = precision_score(labels_all, preds)\n",
    "    recall = recall_score(labels_all, preds)\n",
    "    f1 = f1_score(labels_all, preds)\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2J0P8Z27P5k"
   },
   "outputs": [],
   "source": [
    "def prepare_data(tokenizer, train_ds, val_ds, test_ds, batch_size=16):\n",
    "    train_ds = train_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "    val_ds = val_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "    test_ds = test_ds.map(lambda x: tokenize_function(x, tokenizer), batched=True)\n",
    "\n",
    "    train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    val_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "    test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train and Evaluate with Two Encoder LLMs**\n",
    "\n",
    " Here, we are running experiments with two models:\n",
    "\n",
    " - Model 1: DistilBERT with a hidden size of 768.\n",
    " - Model 2: TinyBERT with a hidden size of 312.\n",
    "\n",
    "\n",
    " The process is to load the corresponding tokenizer and tokenize the datasets, then create DataLoaders for train, validation, and test splits. After that, train classifier head and evaluate on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412,
     "referenced_widgets": [
      "a4b463c2aaa64f46be2bb638d4f68095",
      "9b0aecc851834353bf1ebb26e1fb19f5",
      "5cd657355837487cb9af9cb102dd9ad1",
      "b47783b18b734db5a2fa7e32d6c731c7",
      "0dc256681f7243e1a015f7852c840628",
      "0b9c47c88ff74670932f42ebae674c19",
      "a387300a55c0427eae4323d3a67292bd",
      "4ad8cf565a32489590832c0894839683",
      "ef981b0524444bbdb3b66aa500491438",
      "c0f5c010a2674479b71058f9f7aef8fc",
      "d03b5fc50b7c4e7894242a7e4194d6c3",
      "0ab141ce11bc415e92472522e8ed42f9",
      "183eed24a91b4e6faa12897726f1e8a9",
      "5b40381d5c4444c4a976d1937fe39991",
      "09622ab8a5544879aa91f482baed1c42",
      "2774e085ec574c54b7007c0f4456313c",
      "ce754367ec5a40af987069c8c4abed93",
      "111e6030ee094a1dae3f67a2d93c91c7",
      "a9428e606c0048b68ee5068b68cd52a2",
      "47c138a96a694b9c955908b07adddfcf",
      "2916aed7b560493b86ba8fd25570e409",
      "edd1b0818c134579938f39310fad25a1",
      "18cc0065cdf5449d8fd496c9352321fd",
      "92617e1b929b4707a209a250ac36f854",
      "73c624a3e187414db3e61248a20021f2",
      "64a4021e6df44063a7ff7a2746edcefe",
      "932ad55261494dfa9a62691c912889b4",
      "d5532af2b43c46dbaf4827ff97c4c666",
      "693f8b5da1ad447ab7c954a010831798",
      "09461a6fe4a64957a7b7a73820e2dce1",
      "d678fc72364b44f5bd0c244a5cd4f5b1",
      "4ae45d81bf0a48dab0209460a6e57242",
      "2d1c88d8ad8e4958a79697e4f40dcaf3",
      "8b2a3eb4c9c34c4ab0333b65eb7f3054",
      "5d85925ce90e4bb8aed445d6c960aeeb",
      "180a3b5670724515b400d00757f8e92e",
      "00e81cab876d49fca5339444c7bad7ef",
      "08e4393e88894e4b9750eb978a25f726",
      "83c9771d1c5b4295b23e5379c2cbe974",
      "f7c3863b15604b7cae6f10ba604b6c26",
      "8b70a4d810234b9c92ed1dbdf7c8e567",
      "04376e4e47ea45a7be7cf99517c906f7",
      "c673a919e515414c932908e871ba9a52",
      "3a3c9afa917a428a88d3c0a4eab3a4af",
      "51cafa2bda864ec894fb8f533aaa9575",
      "2753f2a4480944c5b50debc7500deeaa",
      "34b1e8977cc14b9ba33ce3e751ecb4a2",
      "f55102ae6a38485a967fef82ba8e5aab",
      "58a0769b67d34c59b5cd8a969e37e256",
      "c62a12f8487942dab55f2afd8bf6abbf",
      "75ca588249ec4f7e9902d90c00f47673",
      "639858dfe9df4a59b8567bffee5ea073",
      "229506fe3ff64652b6ed692ef3934271",
      "6b3daa8243ce46b6b7f3a1305e3da590",
      "979e11f12ab04fb3a5e9c53ea84d5bfe",
      "408e807f05cd4266a51f2dbaa1658d9f",
      "b927dd07c04848269fc594bb6eeef096",
      "9dae0952ee0a4d03a1d657a7ff093d1f",
      "4e4975c10dbd43a7b15aed027d2d0081",
      "eeaac55865b543c096a4937cf3bf68ae",
      "6ff786278fa64bcdb00ae173bd4d8ec9",
      "6631dd1e788845b58e254cca0714b209",
      "f76c8aa2b1694464a86d68cb3c0a6197",
      "33558cd72b9c407286c8251dcbb01efe",
      "e3111800c85c4dc9af6c57abbc6c652b",
      "3e7ed4e5a72e4773a093a667bb81793e",
      "78d6ce4944a14bd8a92ddbfd5e04f332",
      "fbc4a834db294fe39d061552637796ec",
      "c887256f8aee435f97e2d78de1d3b346",
      "8b57b960031e42e08cb60ca5279376e0",
      "7e749a1e85d74da587b91ea45c93b115",
      "8e1e8c05c1dd4150a833b7098421ebfc",
      "31f900550fa148788d9d460f43ce4223",
      "4b426b71ffb3435cb71b7fb1b0559590",
      "835cf529f7974a2085b3b08e0675a623",
      "dac022fa00aa44fca4969e1d589d0da7",
      "eec891340e6b47d8b329f6f3fbe41940",
      "3b64fb17261245118c9c8620abde312f",
      "f8fd58c623fe4d18baf2a21d85e0aa7d",
      "6dc2446a5153455fad9056e52a0c11e0",
      "20cf62241ecb458b839c87c3cc3879e9",
      "490fe1a4efcd44a78ab829a45fb78b89",
      "adaa345ab8ea49669e3c3df1b18ffdc1",
      "eb94fd5bf17d45b291f8b4e0c9a3da1b",
      "e2292f3ecb33498e856c7859ce457260",
      "77ab98efd0b440b3a5ac118d1d759d03",
      "24f35413d7044176bc403eb098db8b14",
      "65c38a7ec9464b85a628f3c32b402df0"
     ]
    },
    "id": "s9D9MZF67Tl_",
    "outputId": "68a0e4bf-b1d3-4e05-ddf3-bf576d503f2b"
   },
   "outputs": [],
   "source": [
    "print(\"\\nExperiment 1: DistilBERT\")\n",
    "distilbert_model_name = \"distilbert-base-uncased\"\n",
    "distilbert_hidden_size = 768\n",
    "tokenizer_distilbert = AutoTokenizer.from_pretrained(distilbert_model_name)\n",
    "\n",
    "train_loader_distilbert, val_loader_distilbert, test_loader_distilbert = prepare_data(tokenizer_distilbert, train_dataset, val_dataset, test_dataset, batch_size=16)\n",
    "model_distilbert = CustomClassifier(distilbert_model_name, hidden_size=distilbert_hidden_size, dropout_rate=0.1)\n",
    "\n",
    "model_distilbert = train_model(model_distilbert, train_loader_distilbert, val_loader_distilbert, epochs=3, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4tnd3FsM7cq7",
    "outputId": "1e552e8c-cf13-458d-ba2a-efa1514b541a"
   },
   "outputs": [],
   "source": [
    "test_metrics_distilbert = evaluate_model(model_distilbert, test_loader_distilbert)\n",
    "print(\"\\nDistilBERT Test Metrics:\")\n",
    "print(test_metrics_distilbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326,
     "referenced_widgets": [
      "591a5a148b814851b0aa623f4dad6608",
      "403c3fc7c2ee4730912d9d0aa3db66c0",
      "44e2ee4bb6ac4c6e886c5aa6171c4c0c",
      "d39d0d85ae734f0da7a0a2a117e11d20",
      "25b3d976a8fe4436a7ac494f6c27f366",
      "e5d91ca6e5804e1ab3d04075801c835d",
      "91ef60d4ead74866b1877bbd6e548bb2",
      "6ae2a8fa565f4454940455e6fc8cc313",
      "e75915a01c574a6aab520e54dbc110f6",
      "d8677601fef94b79b0516f549fce86a6",
      "6bc6df1db576498ab254ec329bf8a071",
      "211d05a5bb654e15b7bc08654cf15464",
      "e7424e9ca2bf4bbb942e738a3e513067",
      "22549a68ff8143bfa38d11601f454802",
      "21c905d41210434a928c27327ed1fb99",
      "d0a1f705883040abb0b1a8d580affc21",
      "b2d7773b3f4948c6848a6a0438152d58",
      "fd5e9e9a7e484b118b90d48c1af8f7c2",
      "fe1951947ae04b009f25eb6e2317d385",
      "30ad482a055a40f08efdba80fe98e944",
      "d635d2aa53c345fc8d55ef058dfcc776",
      "7bb285326d304f3bbb444319e11d051b",
      "cc158c732ba04b14aceef0c678104f18",
      "b867382ab33c4ebcaeefc0f0b650dde2",
      "1fcef00f6f2c45e585be7764c9cfab90",
      "11d93ab4811c4101a240db94fcb8110d",
      "7beb74f775e7449d9e25e3d905286015",
      "9f272a33f9ed48aa821c1da187119296",
      "44c69c9033694ca789f474b71489b0b7",
      "02bc114028a243a88de2d0559e332114",
      "5e9c4e24ca144a5b80e879f7475a59b2",
      "7aff9d946f964404a9a30edead3e2bde",
      "e843fc42edd44c35a5f2dd361dd944e3",
      "251d485ff0dc407b8ee4e346dc2379f0",
      "96d8e9bc834a42b2bc9573b649de06d6",
      "92a0c5d6d7074377ae5860fd7fff2bb0",
      "756ede885b0c4081a98701523b62d6fe",
      "4fb61797afbd42e3a1aee6c6623abc60",
      "41b11e729db24a84be93d1cdc25361e8",
      "c3f901b70c2c49d2a48fd0cbf41ad478",
      "a0dddc40f0fb456284ebdbd9f72cc8f2",
      "332f52b6aeeb45e0acf3f43906de0ab0",
      "b5949230842343ee81b96064c69d3fd0",
      "e5e7e6e9b2b14225812a8620e31e6ac3",
      "71f74ce5bd0a4a269f75117a133e844b",
      "3ef7178c84434993abfaddf61e1e7c21",
      "a0221fce2e4148ddbc627bbae7c5ca3e",
      "0579c7b18fec4ae99dddc28c9ca6eb1f",
      "3b9a44a36e4240828cf64605ac7f032c",
      "e6de1b84c75a462498afcc24ca06562a",
      "befcf9c8024c4fa1b2027eab1eca46bb",
      "db3658b6b2a144a4a163b6f528a7a758",
      "e89081b7d0724558aa06859db6d9d43d",
      "4d495c5ece6d4b55b76ac4d01515e73e",
      "6010e403917c46749870ae63f639e2d7",
      "a5ed3262228a4efe8369d85b0d79d590",
      "47ce37091092467db723979b79221f42",
      "ce05df4319cd4d6e8e33a0b141561ba0",
      "0ebbb96605d047d68774d1bf2bb598ee",
      "92bcbcfc44624a2489139b96a8b8b595",
      "e2979b3a1f6d4d678f66448b7a7b23f3",
      "4830e488ab8946bd86f8e725b79473e8",
      "27204171f5d147bb96e59476212d167a",
      "e093302c30844564a80592b7c3444386",
      "53c022f7d79f499c85583dbe9c18852b",
      "cfc2858cf4764b39949d3807b939e11e",
      "b639c45237d44aa6b43298e9c7b65427",
      "0a1615d59d014ca392b4c145f8c534e0",
      "967c175048af48f489cdddabe0811e10",
      "8f48b1b8485e44aaa996d185a6387f27",
      "3691fac79c8e488684298ead547b45b0",
      "8da4168d586043ab86dc330102b15004",
      "eb80d45191eb4c0592db9c83ee264118",
      "1fb44ae965084279a2231cfe7e179948",
      "68033f52610c44fc87a6c1e2b9407276",
      "c09b07ca64874aaf8da944bd2056cbd9",
      "05d52cf96d564c55bfce64f4bc926c70"
     ]
    },
    "id": "TTmM6iY47fnv",
    "outputId": "7a0fe084-1cee-4395-e804-0ebd1b52e8fb"
   },
   "outputs": [],
   "source": [
    "print(\"\\nExperiment 2: TinyBERT\")\n",
    "tinybert_model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
    "tinybert_hidden_size = 312\n",
    "tokenizer_tinybert = AutoTokenizer.from_pretrained(tinybert_model_name)\n",
    "\n",
    "train_loader_tinybert, val_loader_tinybert, test_loader_tinybert = prepare_data(tokenizer_tinybert, train_dataset, val_dataset, test_dataset, batch_size=16)\n",
    "model_tinybert = CustomClassifier(tinybert_model_name, hidden_size=tinybert_hidden_size, dropout_rate=0.1)\n",
    "\n",
    "model_tinybert = train_model(model_tinybert, train_loader_tinybert, val_loader_tinybert, epochs=3, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mXFxeTG87kX7",
    "outputId": "68860a27-2b9b-4f18-ad48-13d2336b9ade"
   },
   "outputs": [],
   "source": [
    "test_metrics_tinybert = evaluate_model(model_tinybert, test_loader_tinybert)\n",
    "print(\"\\nTinyBERT Test Metrics:\")\n",
    "print(test_metrics_tinybert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1ywSr5u8M4a"
   },
   "source": [
    "**Evaluation Summary**\n",
    "\n",
    "**DistilBERT:**  \n",
    "The DistilBERT model shows very good learning curves and the validation accuracy is at 98.05% by the final epoch. Its test performance is also outstanding with 98.3% accuracy, 98.4% precision, 98.2% recall, and a 98.3% F1 score. These results clearly shows that the model is excellent in capturing details of the spam classification task.\n",
    "\n",
    "**TinyBERT:**  \n",
    "The TinyBERT model was started with a higher training loss but the validation accuracy improved and finally it is 93.47% at the third epoch. Its test metrics are 93.95% accuracy, 92.6% precision, 95.6% recall, and 94.1% F1 score. This is clearly very good performance even if slighly lower than DistilBERT. As it is a lightweight it is good when resources are less.\n",
    "\n",
    "So, both models shows excellent performance for spam classification. But for this assignment task we are choosing DistilBERT as it shows higher performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving weights of both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1Qh1EOP5my1"
   },
   "outputs": [],
   "source": [
    "torch.save(model_distilbert.state_dict(), \"distilbert_classifier.pt\")\n",
    "torch.save(model_tinybert.state_dict(), \"tinybert_classifier.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- Enron Spam Dataset: https://huggingface.co/datasets/SetFit/enron_spam\n",
    "- DistilBERT Documentation: https://huggingface.co/docs/transformers/en/model_doc/distilbert\n",
    "- TinyBERT Documentation: https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D\n",
    "- PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "- Transformers Library: https://huggingface.co/transformers/\n",
    "- Datasets Library: https://huggingface.co/docs/datasets/\n",
    "- scikit-learn: https://scikit-learn.org/stable/user_guide.html\n",
    "- NumPy Documentation: https://numpy.org/doc/stable/user/index.html#user\n",
    "- Python random module Documentation: https://docs.python.org/3/library/random.html"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

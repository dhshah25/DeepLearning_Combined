{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I4tQIOgvEPTk",
    "outputId": "28320b29-00bf-4d88-f910-3273991458c7"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6221MqRUQgC"
   },
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tkctW9xUNDN"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support, roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TAX1yIEYf36"
   },
   "source": [
    "Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9P2gMXcYkes"
   },
   "outputs": [],
   "source": [
    "train_df=pd.read_csv('train.csv', names=['label', 'title', 'description'])\n",
    "test_df=pd.read_csv('test.csv', names=['label', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rm8OTPoAP4c"
   },
   "outputs": [],
   "source": [
    "train_df['text']=train_df['title']+\" \"+train_df['description']\n",
    "test_df['text']=test_df['title']+\" \"+test_df['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4n3PJKJ-lrKc"
   },
   "outputs": [],
   "source": [
    "train_df['label']-=1\n",
    "test_df['label']-=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eK3R2q2XDPpa"
   },
   "source": [
    "Dataset Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z35vplxnDVbL",
    "outputId": "77eb696e-7b3a-4669-b6e0-31f167bbf9d0"
   },
   "outputs": [],
   "source": [
    "print(f'The total no: of training samples are: {len(train_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwrDFf97FKBK",
    "outputId": "5f62513f-455b-4477-c240-6784485f8586"
   },
   "outputs": [],
   "source": [
    "print(f'The total no: of testing samples are: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B8fOKLknDrog",
    "outputId": "b2143f2f-6ba9-4923-d06c-dfce52bdd2e4"
   },
   "outputs": [],
   "source": [
    "print(f'Class Distribution:')\n",
    "print(train_df['label'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7SNOER6PEWLN",
    "outputId": "c9ef8550-4068-4f47-81bf-3e8618c7c541"
   },
   "outputs": [],
   "source": [
    "train_df['word_count']=train_df['text'].apply(lambda x: len(x.split()))\n",
    "print('Statistics Pertaining to Word Count:')\n",
    "print(train_df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4JuXvWrFpQC"
   },
   "source": [
    "**Dataset Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "ML2XrqsRFudf",
    "outputId": "958055e3-1e5a-4909-ee72-63c3fa8437b3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=train_df, x='label')\n",
    "plt.title('Training Data Class Distribution')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "THISrTd8GJrP",
    "outputId": "481eb396-f7a7-46ef-ab91-cf3a27393ae8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train_df['word_count'], bins=50, kde=True)\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('No: of Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Akt8QUCiow80",
    "outputId": "b490273b-532e-40f6-feac-b02940ea75ab"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oanCwJ-dGqAR"
   },
   "outputs": [],
   "source": [
    "all_words=' '.join(train_df['text'].tolist()).lower()\n",
    "all_words=re.sub(r'[^a-zA-Z0-9\\s]', '', all_words)\n",
    "tokens=[word for word in word_tokenize(all_words) if word not in stopwords.words('english')]\n",
    "frequent_words=Counter(tokens).most_common(20)\n",
    "words, frequency=zip(*frequent_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JbRpZiHKHyMG",
    "outputId": "a4fd583a-677a-42f7-9d19-2cd1d0d133ab"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 14))\n",
    "sns.barplot(x=list(words), y=list(frequency))\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4WaYLSUmj8g"
   },
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1oo6l6HmnRo"
   },
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "  text=re.sub(r'[^A-Za-z0-9\\s]', '', text)\n",
    "  text=text.lower()\n",
    "  tokens=word_tokenize(text)\n",
    "  tokens=[word for word in tokens if word not in stopwords.words('english')]\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkgqwRBFpmDi"
   },
   "outputs": [],
   "source": [
    "train_df['tokens']=train_df['text'].apply(preprocessing)\n",
    "test_df['tokens']=test_df['text'].apply(preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su5IIGR5rXbO"
   },
   "outputs": [],
   "source": [
    "counter=Counter()\n",
    "for tokens in train_df['tokens']:\n",
    "  counter.update(tokens)\n",
    "vocab={word: idx+1 for idx, (word, _)in enumerate(counter.most_common())}\n",
    "vocab_size=len(vocab)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhnlGhbIxBIl"
   },
   "outputs": [],
   "source": [
    "def convert_to_numerical(tokens, vocab):\n",
    "  return [vocab[word] for word in tokens if word in vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUoFkUiFxWxn"
   },
   "outputs": [],
   "source": [
    "train_df['token_ids']=train_df['tokens'].apply(lambda x: convert_to_numerical(x, vocab))\n",
    "test_df['token_ids']=test_df['tokens'].apply(lambda x: convert_to_numerical(x, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH_zloKEzBXa"
   },
   "source": [
    "Creating a PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9WiTEBlxykUF"
   },
   "outputs": [],
   "source": [
    "class AGNewsDataset(Dataset):\n",
    "  def __init__(self, texts, labels, max_length=256):\n",
    "    self.texts=[t[:max_length]+[0]*(max_length-len(t)) for t in texts]\n",
    "    self.labels=labels\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return torch.tensor(self.texts[idx]), torch.tensor(self.labels[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke13Tqy62N8e"
   },
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels=train_test_split(train_df['token_ids'].tolist(), train_df['label'].tolist())\n",
    "\n",
    "train_dataset=AGNewsDataset(train_texts, train_labels)\n",
    "val_dataset=AGNewsDataset(val_texts, val_labels)\n",
    "test_dataset=AGNewsDataset(test_df['token_ids'].tolist(), test_df['label'].tolist())\n",
    "\n",
    "train_loader=DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader=DataLoader(val_dataset, batch_size=32)\n",
    "test_loader=DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_B4ST1W4MlH"
   },
   "source": [
    "**Defining Transformer Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJvqqUsA4iPg"
   },
   "outputs": [],
   "source": [
    "class Encoding(nn.Module):\n",
    "  def __init__(self, d_model, max_len=256):\n",
    "    super().__init__()\n",
    "    pe=torch.zeros(max_len, d_model)\n",
    "    position=torch.arange(0, max_len).unsqueeze(1)\n",
    "    div_term=torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe[:, 0::2]=torch.sin(position * div_term)\n",
    "    pe[:, 1::2]=torch.cos(position * div_term)\n",
    "    self.pe=pe.unsqueeze(0)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return x+self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, num_classes=4, max_len=256):\n",
    "    super().__init__()\n",
    "    self.embedding=nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "    self.pos_encoder=Encoding(embed_dim, max_len)\n",
    "    encoder_layers=nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=512)\n",
    "    self.transformer_encoder=nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "    self.classifier=nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "  def forward(self, src):\n",
    "    x=self.embedding(src)\n",
    "    x=self.pos_encoder(x)\n",
    "    x=x.permute(1, 0, 2)\n",
    "    x=self.transformer_encoder(x)\n",
    "    x=x.mean(dim=0)\n",
    "    return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSbzOMQkIhkP"
   },
   "source": [
    "The transformer architecture I've defined is an encoder-based setup. Since, the task at hand is a classification one, I don't make use of a decoder.\n",
    "\n",
    "Firstly, I start with embeddings, to which I add positional encoding that helps the model keep track of the order of words. The positional encoding I've implemented is in line with what has been outlined in the original paper (sine/cosine based).\n",
    "\n",
    "Next, at its core I implemented a stack of 2 transformer encoder layers (TransformerEncoderLayer instances), each with multi-head self-attention and a feedforward block. Additionally, layer normalization allows for stabilization of training. The attention mechanism enables the capturing on long range dependencies.\n",
    "\n",
    "Finally, I take the mean of the ouputs (average pooling) to get a fixed-size vector which is then passed through a fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_C0zxOo55h6i",
    "outputId": "79b7cd2f-b495-4179-aa53-bb5df594889e"
   },
   "outputs": [],
   "source": [
    "model=Transformer(vocab_size=vocab_size)\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3d4TBt7EnqC"
   },
   "source": [
    "Defining the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eAshtltKAa4"
   },
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQMV7Uv7EqRf"
   },
   "outputs": [],
   "source": [
    "def train(epochs=5):\n",
    "  epochs=epochs\n",
    "  epoch_loss=0.0\n",
    "  train_accuracies, val_accuracies=[], []\n",
    "  train_losses, val_losses=[], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train() #Set model to training mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss=0.0\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "      texts, labels=texts.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs=model(texts)\n",
    "      loss=loss_fn(outputs, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss+=loss.item()\n",
    "\n",
    "      total+=labels.size(0)\n",
    "      correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "    epoch_loss=running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    accuracy=100*(correct/total)\n",
    "    train_accuracies.append(accuracy)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.2f}')\n",
    "\n",
    "    model.eval() #Set model to evaluation mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "      for texts, labels in val_loader:\n",
    "        texts, labels=texts.to(device), labels.to(device)\n",
    "        outputs=model(texts)\n",
    "        loss=loss_fn(outputs, labels)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "        val_loss+=loss.item()\n",
    "\n",
    "    accuracy=100*(correct/total)\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    print(f'The accuracy of the model is: {accuracy:.2f}%')\n",
    "\n",
    "  return train_accuracies, val_accuracies, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-Fy_4wz8ck7"
   },
   "outputs": [],
   "source": [
    "accuracy_metric=MulticlassAccuracy(num_classes=4).to(device)\n",
    "\n",
    "def evaluate():\n",
    "  model.eval()\n",
    "  all_preds=[]\n",
    "  all_labels=[]\n",
    "  all_probs=[]\n",
    "  total_loss=0.0\n",
    "  total_samples=0\n",
    "  with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "      inputs, labels=inputs.to(device), labels.to(device)\n",
    "      outputs=model(inputs)\n",
    "\n",
    "      loss=loss_fn(outputs, labels)\n",
    "      total_loss+=loss.item()*inputs.size(0)\n",
    "      total_samples+=inputs.size(0)\n",
    "\n",
    "      probs=F.softmax(outputs, dim=1)\n",
    "      preds=torch.argmax(probs, dim=1)\n",
    "\n",
    "      all_preds.extend(preds.cpu().numpy())\n",
    "      all_probs.extend(probs.cpu().numpy())\n",
    "      all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  avg_loss=total_loss/total_samples\n",
    "  print(f'The loss obtained on the model is: {avg_loss:.2f}')\n",
    "\n",
    "  all_preds_tensor=torch.tensor(all_preds, dtype=torch.int64).to(device)\n",
    "  all_labels_tensor=torch.tensor(all_labels, dtype=torch.int64).to(device)\n",
    "\n",
    "  computed_accuracy=accuracy_metric(all_preds_tensor, all_labels_tensor)\n",
    "  print(f'The accuracy obtained on the model is: {computed_accuracy.item()*100:.2f}%')\n",
    "\n",
    "  precision, recall, fscore, _=precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "  print(f'For the model - precision:{precision:.2f}, recall: {recall:.2f}, F-score: {fscore:.2f}')\n",
    "\n",
    "  return computed_accuracy.cpu().numpy(), avg_loss, all_labels, all_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7UqRsTyKI8K",
    "outputId": "9d734902-20e8-470e-f87f-8fb1efe5719c"
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "train_accuracies, val_accuracies, train_losses, val_losses=train()\n",
    "base_model_time=time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aB7ox2ssHsHC",
    "outputId": "32e384b8-686f-4d20-9da2-caa1936aec60"
   },
   "outputs": [],
   "source": [
    "test_accuracy, test_loss, all_labels, all_probs=evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "up6DchHzH2Mz"
   },
   "source": [
    "**Model Optimization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwsK423MH45w"
   },
   "source": [
    "Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6im54iLWH3k5"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim=128, num_heads=4, num_layers=2, num_classes=4, max_len=256):\n",
    "    super().__init__()\n",
    "    self.embedding=nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "    self.pos_encoder=Encoding(embed_dim, max_len)\n",
    "    encoder_layers=nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=512, dropout=0.1)\n",
    "    self.transformer_encoder=nn.TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
    "    self.classifier=nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "  def forward(self, src):\n",
    "    x=self.embedding(src)\n",
    "    x=self.pos_encoder(x)\n",
    "    x=x.permute(1, 0, 2)  # Required shape for Transformer [seq_len, batch_size, embed_dim]\n",
    "    x=self.transformer_encoder(x)\n",
    "    x=x.mean(dim=0)  # Average pooling over the sequence length\n",
    "    return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovaO2vt3IQJG",
    "outputId": "3e11654e-d6a9-435f-a9f6-30eefec64d61"
   },
   "outputs": [],
   "source": [
    "model=Transformer(vocab_size=vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0HLtxw5-z85"
   },
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PMvxsxKuIYtL",
    "outputId": "40432dca-b2d6-430a-e874-d8e3ae13654a"
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "train_accuracies_dropout, val_accuracies_dropout, train_losses_dropout, val_losses_dropout=train()\n",
    "dropout_model_time=time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wj6ErlnkIfEc",
    "outputId": "72cec1bf-55a5-4b15-a3fd-72af7e11d0e1"
   },
   "outputs": [],
   "source": [
    "test_accuracy_dropout, test_loss_dropout, all_labels, all_probs=evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-mW2vPLIX3g"
   },
   "source": [
    "L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwwX502x-3ZM",
    "outputId": "a2a8a77b-c872-49ea-e092-241adcf245ed"
   },
   "outputs": [],
   "source": [
    "model=Transformer(vocab_size=vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALotj9BEIW7B"
   },
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjvW0BgGp2S-"
   },
   "outputs": [],
   "source": [
    "def train(epochs=5):\n",
    "  epochs=epochs\n",
    "  epoch_loss=0.0\n",
    "  current_best_accuracy=0.0\n",
    "  train_accuracies, val_accuracies=[], []\n",
    "  train_losses, val_losses=[], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train() #Set model to training mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss=0.0\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "      texts, labels=texts.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs=model(texts)\n",
    "      loss=loss_fn(outputs, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss+=loss.item()\n",
    "\n",
    "      total+=labels.size(0)\n",
    "      correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "    epoch_loss=running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    accuracy=100*(correct/total)\n",
    "    train_accuracies.append(accuracy)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.2f}')\n",
    "\n",
    "    model.eval() #Set model to evaluation mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "      for texts, labels in val_loader:\n",
    "        texts, labels=texts.to(device), labels.to(device)\n",
    "        outputs=model(texts)\n",
    "        loss=loss_fn(outputs, labels)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "        val_loss+=loss.item()\n",
    "\n",
    "    accuracy=100*(correct/total)\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    print(f'The accuracy of the model is: {accuracy:.2f}%')\n",
    "\n",
    "    if accuracy>current_best_accuracy:\n",
    "      current_best_accuracy=accuracy\n",
    "      torch.save(model.state_dict(), 'a2_part_3_dshah22_ramasair.pth')\n",
    "\n",
    "  return train_accuracies, val_accuracies, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K1pDce2_JCF5",
    "outputId": "1966eed4-adc4-48c6-a837-fc889258ee2e"
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "train_accuracies_l2, val_accuracies_l2, train_losses_l2, val_losses_l2=train()\n",
    "l2_model_time=time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iV2cJEz1JHd5",
    "outputId": "7dd42e4f-ed51-4727-d280-db7863b922d5"
   },
   "outputs": [],
   "source": [
    "test_accuracy_l2, test_loss_l2, all_labels, all_probs=evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBSIl1AKJba4"
   },
   "source": [
    "Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Iq8tBxk-88h",
    "outputId": "96a5d2c3-a97d-4bd9-e796-d74c1d3c13b4"
   },
   "outputs": [],
   "source": [
    "model=Transformer(vocab_size=vocab_size)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ohDy7bk--xI"
   },
   "outputs": [],
   "source": [
    "loss_fn=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5S0acBXJpAq"
   },
   "outputs": [],
   "source": [
    "scheduler=torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bjA3waTJ8br"
   },
   "outputs": [],
   "source": [
    "def train(epochs=5):\n",
    "  epochs=epochs\n",
    "  epoch_loss=0.0\n",
    "  train_accuracies, val_accuracies=[], []\n",
    "  train_losses, val_losses=[], []\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    model.train() #Set model to training mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    running_loss=0.0\n",
    "\n",
    "    for texts, labels in train_loader:\n",
    "      texts, labels=texts.to(device), labels.to(device)\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs=model(texts)\n",
    "      loss=loss_fn(outputs, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      running_loss+=loss.item()\n",
    "\n",
    "      total+=labels.size(0)\n",
    "      correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "    epoch_loss=running_loss/len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "    accuracy=100*(correct/total)\n",
    "    train_accuracies.append(accuracy)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.2f}')\n",
    "    scheduler.step()\n",
    "\n",
    "    model.eval() #Set model to evaluation mode\n",
    "    correct=0\n",
    "    total=0\n",
    "    val_loss=0.0\n",
    "    with torch.no_grad():\n",
    "      for texts, labels in val_loader:\n",
    "        texts, labels=texts.to(device), labels.to(device)\n",
    "        outputs=model(texts)\n",
    "        loss=loss_fn(outputs, labels)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(outputs.argmax(1)==labels).sum().item()\n",
    "\n",
    "        val_loss+=loss.item()\n",
    "\n",
    "    accuracy=100*(correct/total)\n",
    "    val_accuracies.append(accuracy)\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    print(f'The accuracy of the model is: {accuracy:.2f}%')\n",
    "\n",
    "  return train_accuracies, val_accuracies, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSNKrzkAKHaj",
    "outputId": "2ee2271d-48d5-47c1-bab8-c00e9b5299ac"
   },
   "outputs": [],
   "source": [
    "start_time=time.time()\n",
    "train_accuracies_lr, val_accuracies_lr, train_losses_lr, val_losses_lr=train()\n",
    "lr_model_time=time.time()-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IXkmkGUAKKVu",
    "outputId": "8bc0b1f4-9b5d-43ba-f16b-004a2a627d2c"
   },
   "outputs": [],
   "source": [
    "test_accuracy_lr, test_loss_lr, all_labels, all_probs=evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ihafgrPM99uy"
   },
   "outputs": [],
   "source": [
    "train_accuracies_best, val_accuracies_best, train_losses_best, val_losses_best=train_accuracies_lr, val_accuracies_lr, train_losses_lr, val_losses_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrMye4b_-bjq"
   },
   "outputs": [],
   "source": [
    "test_accuracy_best=test_accuracy_lr\n",
    "test_loss_best=test_loss_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgPPhOlCK8U3"
   },
   "source": [
    "To improve the performance of the transformer model, I explored employing dropout, l2 regularization, and learning rate scheduler. Implementing dropout within the core transformer architecture, we notice no significant improvement in the performance. Next, I employed a combination of dropout and l2 regularization which resulted in a slight improvement. Lastly, a combination of dropout, l2 regularization, and learning rate scheduler results in a similar performance as the combination of dropout and l2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCgMtSu1li-j"
   },
   "source": [
    "**Results Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KPnUtNPCpGx",
    "outputId": "ce616f0d-8956-47bd-f8bb-8fc8f3c238e8"
   },
   "outputs": [],
   "source": [
    "print(test_accuracy_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HHpiHyJxlp0V",
    "outputId": "82e6d6da-a000-42ad-a1d9-2a139ea013ae"
   },
   "outputs": [],
   "source": [
    "fig, axs=plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "epochs=5\n",
    "\n",
    "all_labels_encoded=label_binarize(all_labels, classes=[0, 1, 2, 3])\n",
    "all_probs=np.array(all_probs)\n",
    "\n",
    "for i in range(4):\n",
    "  fpr, tpr, _=roc_curve(all_labels_encoded[:, i], all_probs[:, i])\n",
    "  roc_auc=auc(fpr, tpr)\n",
    "\n",
    "axs[0, 0].plot(train_accuracies_best, label='Best Model Training Accuracy')\n",
    "axs[0, 0].plot(train_accuracies, label='Base Model Training Accuracy')\n",
    "axs[0, 0].plot(train_losses_best, label='Best Model Training Loss')\n",
    "axs[0, 0].plot(train_losses, label='Base Model Training Loss')\n",
    "axs[0, 0].plot(val_accuracies_best, label='Best Model Validation Accuracy')\n",
    "axs[0, 0].plot(val_accuracies, label='Base Model Validation Accuracy')\n",
    "axs[0, 0].plot(val_losses_best, label='Best Model Validation Loss')\n",
    "axs[0, 0].plot(val_losses, label='Base Model Validation Loss')\n",
    "axs[0, 0].plot(test_accuracy_best, label='Best Model Testing Accuracy')\n",
    "axs[0, 0].plot(test_accuracy, label='Base Model Testing Accuracy')\n",
    "axs[0, 0].plot(test_loss_best, label='Best Model Testing Loss')\n",
    "axs[0, 0].plot(test_loss, label='Base Model Testing Loss')\n",
    "axs[0, 0].set_title('Comparison of Training, Validation, and Testing Accuracies and Losses (Best vs. Base Model)')\n",
    "axs[0, 0].set_ylabel('Accuracy')\n",
    "axs[0, 0].set_xticks([])\n",
    "axs[0, 0].legend()\n",
    "\n",
    "axs[0, 1].plot(range(1, epochs + 1), train_accuracies_best, label='Tuned Model Training Accuracy')\n",
    "axs[0, 1].plot(range(1, epochs + 1), val_accuracies_best, label='Tuned Model Validation Accuracy')\n",
    "axs[0, 1].set_title('Tuned Model Training and Validation Accuracy Over Time (Epochs)')\n",
    "axs[0, 1].set_xlabel('Epochs')\n",
    "axs[0, 1].set_ylabel('Accuracy')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "axs[1, 0].plot(range(1, epochs + 1), train_losses_best, label='Tuned Model Training Loss')\n",
    "axs[1, 0].plot(range(1, epochs + 1), val_losses_best, label='Tuned Model Validation Loss')\n",
    "axs[1, 0].set_title('Tuned Improved Training and Validation Loss Over Time (Epochs)')\n",
    "axs[1, 0].set_xlabel('Epochs')\n",
    "axs[1, 0].set_ylabel('Accuracy')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "for i in range(4):\n",
    "  axs[1, 1].plot(fpr, tpr, label=f'Class {i} ROC Curve (AUC={roc_auc:.2f})')\n",
    "\n",
    "axs[1, 1].plot([0, 1], [0, 1], 'k--')\n",
    "axs[1, 1].set_title('ROC Curve')\n",
    "axs[1, 1].set_xlabel('False Positive Rate')\n",
    "axs[1, 1].set_ylabel('True Positive Rate')\n",
    "axs[1, 1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a28fELX2O_CB"
   },
   "source": [
    "Visualizing the results, we notice that the base model and the best model (tuned with a combination of dropout and l2 regularization) vary in terms of the obtained accuracy and loss by a slight margin, with the best model performing better over the base. The best model converges on the training and testing data over epochs, resulting in good validation accuracy. A high validation accuracy and a low validation loss suggest that the model is able to generalize well on unseen data, effectively capturing meaningful patterns in the text and classify properly.\n",
    "\n",
    "Additionally, the combination of dropout and l2 regularization helped prevent overfitting, smoothing out the convergence of the model on the data. The metrics obtained indicate that the defined transformer architecture is robust and well suited for the classification task at hand. Beyond the visualization of results, the calculated metrics (precision, recall, F1-score) helps us understand the model performance better.\n",
    "\n",
    "Finally, the model's (transformer's) architecture being lightweight with 2 encoder layers and an embedding size of 128 keeps the model computationally feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50ZQPC3_Q00K"
   },
   "source": [
    "**References**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NVxHY77Q5HI"
   },
   "source": [
    "\n",
    "\n",
    "- Official 'Attention Is All You Need' paper: [Link to the paper](https://arxiv.org/abs/1706.03762)\n",
    "- Official NLTK documentation: [Link to official NLTK documentation](https://www.nltk.org/)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

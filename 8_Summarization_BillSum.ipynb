{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxFv_wh-G7cE",
    "outputId": "5d2f43e7-289d-4d89-9b3d-430b79d2dfb5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets transformers evaluate sacrebleu bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BdQqnLAbG9c7",
    "outputId": "0f8146db-7322-43a8-9cc5-981784e6c1ee"
   },
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading packages and setting seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mt0VrUTIhk98"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "import evaluate\n",
    "import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxolALyAhm3i"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the Billsum dataset from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601,
     "referenced_widgets": [
      "1b2d8c3e8cb64eaa8a094d20a4bd2ef9",
      "92158b596edb44719805293ea45d8a10",
      "2a4b7594170c44a8bccf4a539b7c3e79",
      "9963b92f93004f4bb3431395ee15f550",
      "e1ccbd66ce6d469f93399992ce58bf69",
      "f3d8bf3b98c14df3bc6575474ced5587",
      "d52d0535c8a04e1cb81cceecfedd464c",
      "ff4cc9fd77334afd9cf05949a02b0b21",
      "f125f085cb9d43e6a8fbab8884ca3f6d",
      "f15b4ad53efb4814b3e344adc04fc76a",
      "9228e6046ee94b6fb8c6330dffe9a90a",
      "a032a67b18e44d53919991c39fe1ad02",
      "5255166635044d588349f0b01483dcac",
      "a7309f796f68478c8a4b109efea361d6",
      "6897cbe22b8f4b20989bb6ea4d47912c",
      "07a15970e87640ee8a23f784db250f31",
      "e1f339061e994cb480131bda192932c6",
      "d6f3bf5d11264735a845b548c8fa607c",
      "911f07403aba44b99db9a1740dfc724c",
      "aacdff8163d34da8b7260217ea00c4a4",
      "8db758a0cc8941dc95b9f1aac27384d3",
      "c3c2ed98f28a4bb9bdd1df60ed1664d7",
      "cc9d98eb5e484dd395ac90aa97f9af4d",
      "1847c0894b2a464e86428875cd0d7f85",
      "6b4cc382d83d46628188cb26c607500f",
      "bfa27d7524b44475904751ce7fc26a72",
      "c98e19fa7d3a4f72ba5930a207cc756e",
      "7d3e588d81394212bfedd93e06958e6a",
      "7a8c2b5b62db4c4582dda347c66c24d8",
      "b7763b277d644f47bee55e23ed053388",
      "92a61c73e50e498083d7bdacdf76765e",
      "9d8b55e1ad4845118fb3f1ca4991004f",
      "d26719621f7a49ad9a6ffad344a90e0b",
      "9f6e6c38fc9e46fe881f6d40eb2550bd",
      "d4df4cac6ff142b6aed22bdab63a3eab",
      "fba0cbb958b346689c6986acfde0dd4d",
      "bf12a77e05b8428b86dc5f9b60b692dc",
      "f9543fbe1a2a4ea0b3397b8485d7f58e",
      "9da3a94cf6aa42b0a7857109dadfb6c4",
      "82d6910646ee48f6a35ce5a3ad5b9b50",
      "6cec20fd619f4b6a98b0a095ab3f547f",
      "6a027e6f3e68423491dc4265756fefb8",
      "e1f5c05a988f48d0b9de742b2e2dfccf",
      "3b2918f2e1aa4380a434baff79c74cc2",
      "51e70ba0e6714cb08166e8a878dec8c3",
      "168cf42653334b138b923f200c881fb2",
      "e0b4aaeb1ccf4fea8b7a0b8b10573461",
      "5bada7a8930d41d3884b7c8f03359110",
      "cb4fa20ee895444d9af97aded7df8ef4",
      "e288cce9084c4e0f871d26d6f2264871",
      "408f13e648094dd6910464ea1d4ee650",
      "720f2540f98e467a884520fd11fc7ccb",
      "c3de83c8fc0d4ae8a70c9d56465c506a",
      "a32260d8803343cca4bc63d196ad0dc9",
      "729c0b8e487d4fa8856cfed923e17d14",
      "c8d6c1790bae4ba2b325047caf2ba573",
      "e65276cf76f54ceba695ec3b72cd2bbc",
      "720bf57915b0476398f09aeff18c36d1",
      "7dc411ec5c0f4a609e8dc2b2d3a2a475",
      "e2e558d095244960bce3436dd343bbf6",
      "15a03a11c2fe423887451b114cd5f9b1",
      "7c6b4958736d4f5889327b3c6dea934d",
      "3f122ae2fc2747ae838b01f691dd6f71",
      "14ab944fd10f4351aeb8fe404bb6d0dc",
      "47df018adff749cb9a50e0c5ef60e907",
      "008bec77e4634d94a5625e6e4adebd93",
      "912f391be3cb4741b44d0cc000cbba19",
      "97181fc12c884ccbb47b8cc5775f16cb",
      "86d4794efe5a4a1ba8473d015720ae65",
      "10c6e57a13964513bacd832afde60a1d",
      "d3bf9a4aebfa4726932d8f95ef21aaff",
      "695c456b778044dfac192c21756a2cc3",
      "06d7b2642cae447a90358ca2014a43bc",
      "8613d374d3b44575927e650ee46e64ef",
      "e66af827e2034cc883ec0d635a5b9f3e",
      "1c512d76f5d84b388b984b14241fc717",
      "215258bb11744aa387c9830e1ecd7d4a"
     ]
    },
    "id": "NJLKiYaXhpYN",
    "outputId": "31bc90cd-1adf-4754-c850-21a2d90aa99c"
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"FiscalNote/billsum\")\n",
    "print(\"Original splits:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of samples for each split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BuNy2PfNhqFi",
    "outputId": "0c5558cb-6621-436a-c1d5-221ce34960e5"
   },
   "outputs": [],
   "source": [
    "for split in dataset.keys():\n",
    "    print(f\"Split: {split}, Number of samples: {len(dataset[split])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a validation split if not available by using a 90/10 split from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ldUkRaBjht-N",
    "outputId": "c5121910-856b-476c-c36a-742c60d36d30"
   },
   "outputs": [],
   "source": [
    "if \"validation\" not in dataset.keys():\n",
    "    train_valid = dataset[\"train\"].train_test_split(test_size=0.1, seed=seed)\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_valid[\"train\"],\n",
    "        \"validation\": train_valid[\"test\"],\n",
    "        \"test\": dataset[\"test\"]\n",
    "    })\n",
    "    print(\"After splitting, splits:\", dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the tokenizer for facebook/bart-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145,
     "referenced_widgets": [
      "e4293d3af91349e996617087ef46f578",
      "fa29286f3be940c295476c1ee75e9aad",
      "118241c737d740cc9f87b298a4213db4",
      "da1fa40e163a4998878c0fb34e4e1e16",
      "c4420549e6744089910a73bbd8a3e0be",
      "5ef42dc827f5443385df2dea7b4f10e2",
      "f6c875484eb84ec3bed01297e669632f",
      "8f76b7ee49c74ebab808f4f75094d8fd",
      "902cf79fa26748a88dce61cab7b636e8",
      "b39d0956429548659666dbcb44a37107",
      "7601d2ad17c74084a522978fa6d129c1",
      "4ee881ad87db45099540e9802911e704",
      "a72c04e7cd7a41b785dfc88aa321fd4d",
      "701d99996e5d4e20a44ebab45588d8f5",
      "b099a24946524a3699a019b213d04648",
      "7421ff6bfa7c4c089c7ce609f12c7fee",
      "f58cb2f52f2a4fa6a2749e658bb65077",
      "342b029c4dfa4d4e99f9bba2e37e5b20",
      "8faec2b1a76246f2a9abaadba51e60d3",
      "fdefdea7ffad4ebeb91a6a22db87e69b",
      "9065a44e39ae43aa9ff6fabfa4a84776",
      "eab4d457f9984a72853850ec248c7e4c",
      "d6ea96190abc43db9e2dc4b5b6edfd19",
      "c98d6545b3b443ab86bcc448a39466ce",
      "fa33deeb064d4387a62927257625fd6e",
      "4727bed68e4d45d59272238c7716efed",
      "dff5f2a7b94f47db972c5252ab36271c",
      "6e1f8b11ff9245928a129afd561b5bd7",
      "13de0ae3eaee406aa720c0d2597e39d6",
      "954f3e98d3ab4fb0a8492a36d4af539b",
      "0e06ef844b6b4f3c8252ccbf79d934a5",
      "a7d3cdf3a2974116a4ae42f0382677e3",
      "138341fa8ed744f3b2ab95068431f0c1",
      "54303e2a0da54dc6820006505ecad700",
      "35ef1f607b17429699836007589229b4",
      "d79e104765b147cf8b15600609133501",
      "6bc4ea85fd4448269ae275f2fe08db2f",
      "590b6726a7c546078ed451c3a6c7bdf4",
      "801672c021814a0c987f96d28ee9025f",
      "de4de200e42d475ba251e9e3bce4a51f",
      "407185dfc1f840328f0fb184030732dc",
      "dae31bb100d54121a942cfeb247a9fdf",
      "ac94163f998c4a599ff76479fa2dc26c",
      "074160e7648e4a7e95709a0662bbaf9f"
     ]
    },
    "id": "QwK2ogBMhv9E",
    "outputId": "8636cba0-a9fe-4631-f360-362ab11d4d02"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/bart-base\"\n",
    "tokenizer = BartTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define maximum lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rnRCb57EhzZ3"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a tokenization function for the dataset then applying tokenization and removing columns no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 167,
     "referenced_widgets": [
      "ced6ca595ab04450943ff865ce8a1c75",
      "bda81c2eff044be38c51e10168139c31",
      "7e71c181089d42c4a29a7e9aa618ee8c",
      "0b96792c419e42a3b55f444c8640f9f3",
      "de62af2924de4a51aace0f1b56abb2b5",
      "cf5ecdbe78e945dda26e32c9bb860c6b",
      "6a991eb70c6f45719334cebcdb42ea20",
      "481475f52d484a589f0f5f97a8a0c4e2",
      "6d67f9361d794c2bb2c6b083fac75f60",
      "6c80ad65a00543e08f4607de17ef88a2",
      "b1e21eac68794246b01c0c9590d3304e",
      "fac10a52e84a4b7aa155333d72e134aa",
      "13fa4457ebbe416bb36d1d7b47274f9b",
      "12863bc30470467f85ccc1a3cb7a559e",
      "5faf2fcb26394fc7917253eef613169e",
      "d5f55aef57874ee8902886313173fdbc",
      "87d8f4cd54e847c7a9b6e009f1c6a4e6",
      "43fcf8f2da0c4b099a2ce155cb1ea5fc",
      "49083b9fe4d846e0b203cd2b9b567f32",
      "a5ab0c8c6bd047b696622ee760977b74",
      "ac0c1ff6430d44babfeadd5a7dcc83a2",
      "f94cd4a9976a4c1b9d8f865c554be4eb",
      "bdbfd4aae30543bf802b213b2c9f8c55",
      "744959f5eed14ede84948a3f30b590bb",
      "3edceebd79624189b7792085e4997ccd",
      "79c35608a5b34a608992e5e994b64c77",
      "446ecf9f09dd4bf49fea6d77aabc5dd4",
      "287ff9f11b814a808bd9d993fe321548",
      "e233c70448494915bfff2f045289fccf",
      "78af7fc1ce89425680adf94f544ede80",
      "b9d4a79959a84f20ac1206d15d804c0f",
      "0888dd425df642e7bdd1074314780d54",
      "a752a88184574895885d4ac73d8a34ff"
     ]
    },
    "id": "OmyzsRewiA8y",
    "outputId": "b1b7f420-c177-4e2c-af69-b15d09586728"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"text\"], max_length=max_input_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\", \"summary\", \"title\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the tokenized dataset locally to avoid reprocessing later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "46ace830a3bf47819a38f383860259ba",
      "bc099a8739a64fcb803cd8081714abc3",
      "92291a409c694c3dad6d14e09521b3a3",
      "91830299b7bb40d38fc2f4499b1c8c17",
      "b322fde07d2947949b417ac7fa15f820",
      "901a44988f2c43058916f5a7ff339a4c",
      "f72582d691314e858dbbc656be94eeb0",
      "2ff7d80028df452abad48f718dbf9f71",
      "6ff576bf577b4f0f8dc38e5109fe1287",
      "aefcc9f2ceee4ba3b7387faf83c50dcd",
      "5caa07f6aba543f9bc295d5d129afe12",
      "cf95be04b0f74a7584ca905f07a30991",
      "ebbea13c48204591a0d7805b5e9a4b88",
      "d7a76022a5d04943adcb538391c9fbcc",
      "7b3bbc411e61450e9234c27cf755e0e8",
      "c927af07be4a4a73ba62bb38c684a561",
      "4e0180c62156404aaa35c17129948675",
      "25b46286d6ca41f5b771ae6fe96f0c8f",
      "8ff404c81c034f7188c2a987a42e702f",
      "fe1c44ffde1b41f388f43c617779ae6f",
      "dd2df4dce9a54d30868d525821ec5be8",
      "128c46f58c824ce48689d92c0bc40b06",
      "84299fec8f7b45c195ff35449c7b47e0",
      "095c7a9c35e54adfbc858d1de8c5712d",
      "74b51d32ccd541a4abf3104c385f5f92",
      "f563c7c2998240a6afd4760b70b1a3be",
      "485c2352f6b24552978e44ba8b722ea1",
      "e25da8e8054842f28f9fc92b61ea4cd0",
      "3178ce368c6d46d6840a67eecb4067d1",
      "3e48375ead0c43cf841d68d0d31f7026",
      "5be58ec4bc334e65a9ba8c2cdbc4b586",
      "6b19b9e4c201426491859bde4872ffd5",
      "1f111b699b7443d28f6c47c20755b619"
     ]
    },
    "id": "Y3896K22iEWw",
    "outputId": "73a3143c-f379-4171-a2e0-8ccb11285184"
   },
   "outputs": [],
   "source": [
    "tokenized_dataset_path = \"./tokenized_billsum\"\n",
    "if not os.path.exists(tokenized_dataset_path):\n",
    "    tokenized_datasets.save_to_disk(tokenized_dataset_path)\n",
    "    print(f\"Tokenized dataset saved to {tokenized_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limit the training set to 1000 samples and validation set to 100 samples due to limited resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwPUXETniRuO"
   },
   "outputs": [],
   "source": [
    "min_train_samples = 1000\n",
    "min_valid_samples = 100\n",
    "if len(tokenized_datasets[\"train\"]) > min_train_samples:\n",
    "    tokenized_datasets[\"train\"] = tokenized_datasets[\"train\"].select(range(min_train_samples))\n",
    "if len(tokenized_datasets[\"validation\"]) > min_valid_samples:\n",
    "    tokenized_datasets[\"validation\"] = tokenized_datasets[\"validation\"].select(range(min_valid_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing Methodology**\n",
    "\n",
    "Here, we have started with the Billsum dataset using the Hugging Face Datasets package. Then, printed the number of samples in each dataset split to get a feel for the dataset’s size and distribution.\n",
    "\n",
    "Next, because the dataset did not include a separate validation set, we have to create one by splitting the training data into 90% for training and 10% for validation.\n",
    "\n",
    "Then, we have prepared the data for our model by tokenizing both the documents and the corresponding summaries and used the BartTokenizer to convert the raw text into token IDs. During tokenization, we have set a maximum input length of 1024 tokens and a target (summary) length of 256 tokens so that our inputs stay within manageable limits. To reduce computational load due to the limited resources available on Colab we have to subsample the dataset to use only 1000 training examples and 100 validation examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutomer trainer that overrides prediction_step to use generate() with tuned parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mz50YYK1iVCL"
   },
   "outputs": [],
   "source": [
    "class CustomSeq2SeqTrainer(Seq2SeqTrainer):\n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        generated_tokens = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=max_target_length,\n",
    "            num_beams=8,\n",
    "            length_penalty=1.5,\n",
    "            no_repeat_ngram_size=3,\n",
    "            min_length=50\n",
    "        )\n",
    "        labels = inputs.get(\"labels\")\n",
    "        loss = None\n",
    "        if not prediction_loss_only:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                loss = outputs.loss\n",
    "        return (loss, generated_tokens, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining evaluation metrics using the evaluate library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "dfa7cc53028149d6b61617bd1e75ea98",
      "f794c2661e4b49f0b4054eab8c010167",
      "86b450ff303e466f8f562c766db0d598",
      "603c58a7f5d4422789c7f16ef6596252",
      "bd6596b7efcd4beeaf898181a9ecf791",
      "155d0b7d6ba54e98b2aaaca37f5fd632",
      "0ca2a692b0bf48d5b9d355116895bc29",
      "a8cd8643ab424bddad7be5c239c48c81",
      "8c394035ec214767b50df489fce1f2a6",
      "facd5b21e43d47b7afb8f13b8e288225",
      "62afc3fc6aca49aba9d7cce292a7ebea",
      "56dc0e9b12944cee843fdd38e8a09b3e",
      "99d1e0863186498387a5a79fdaa232b1",
      "b16ed75f97ea498cb3663e979844ab83",
      "813d0e99d9a84974a8584ee230519289",
      "ddd0afd0243949e3ba54341d3c80fab7",
      "aa0f60d88877434cbe14852256d3b3e6",
      "08cad2bf18bc4362a143c4ccbd4fc398",
      "9bf1077aecee4c3f81354f17f80a4602",
      "2ee27f376146494abff3cf9f2d6069fa",
      "a9415014935a4122babe618b1c1ef17b",
      "1737d37f31a94331b8c293d1969d3b89",
      "b2852c9c3ea946b28a6454603574530f",
      "48dad8ccc5864f90b3e4df13ed12b6af",
      "b397ed5efd804bd88cf888c5d33ba0ed",
      "4436cf68d36441e785abd32d6644a43f",
      "65114cc6b1d741988cdd99809dd33bad",
      "9821d156fa584950bbf444fce9e496e0",
      "ca9afbc995cf4462879bfea0a15affff",
      "dd1c346a338942cf9e02665ee6b3ea71",
      "c3410810b346403fa39e6665872b32ad",
      "40335530be434b11a8d7038d252ee6d0",
      "4f9e7ed83906468490411dc5946b626c"
     ]
    },
    "id": "WZAZXCYYioFs",
    "outputId": "33027ea6-7fb0-4b88-a9b4-30f05a5369c6"
   },
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "sacrebleu_metric = evaluate.load(\"sacrebleu\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Safe decoding function to convert token IDs to strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIdaGgT8iohl"
   },
   "outputs": [],
   "source": [
    "def safe_decode(batch_ids):\n",
    "    decoded_batch = []\n",
    "    for ids in batch_ids:\n",
    "        if isinstance(ids, np.ndarray):\n",
    "            ids = ids.tolist()\n",
    "        tokens = tokenizer.convert_ids_to_tokens(ids, skip_special_tokens=True)\n",
    "        tokens = [t if t is not None else \"\" for t in tokens]\n",
    "        decoded_batch.append(\"\".join(tokens))\n",
    "    return decoded_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute metrics function is for calculating ROUGE, BLEU (using sacreBLEU with exponential smoothing), and BERTScore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-s5Q-qGiuXx"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = safe_decode(predictions)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = safe_decode(labels)\n",
    "\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    bleu = sacrebleu.corpus_bleu(decoded_preds, [[ref] for ref in decoded_labels], smooth_method=\"exp\")\n",
    "    bleu_score = bleu.score\n",
    "    bertscore_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "    avg_bertscore = np.mean(bertscore_result[\"f1\"])\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleu\": bleu_score,\n",
    "        \"bertscore\": avg_bertscore\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define 3 different experiment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IC4Fl1Qui2R0"
   },
   "outputs": [],
   "source": [
    "experiment_configs = [\n",
    "    {\"name\": \"config1\", \"learning_rate\": 5e-5, \"train_batch_size\": 2, \"num_train_epochs\": 3},\n",
    "    {\"name\": \"config2\", \"learning_rate\": 3e-5, \"train_batch_size\": 2, \"num_train_epochs\": 3},\n",
    "    {\"name\": \"config3\", \"learning_rate\": 5e-5, \"train_batch_size\": 4, \"num_train_epochs\": 3},\n",
    "]\n",
    "\n",
    "experiment_results = {}\n",
    "trained_trainers = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data collator to pad sequences dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ff979a299d4644f081150d92c584b580",
      "de15d9155a014b8cb380d7ef90efc98a",
      "c98ae46882684be58f03424fbfa4c8ca",
      "d3bec72bf96042cd84621494828c824e",
      "7742fe390926439680cc6d939bf5ccb0",
      "42d13b990e7644a28e619c77ae5461b1",
      "028b19a7175e48b4b05724ef461f714a",
      "fa8b78fb95b34cacab65c5dc107b1b2a",
      "ca81195a0e214833b4bb9bc32fc0d287",
      "4173dc6e6f6841a28e872625b14548b6",
      "25013e6be5b9461e97a4cca0a04a1652"
     ]
    },
    "id": "p-6iamtKi5xc",
    "outputId": "b3d9fa6d-1fdd-42c6-8317-b3d2c55f068e"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=BartForConditionalGeneration.from_pretrained(model_checkpoint),\n",
    "    padding=\"longest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration 1 (Config1):\n",
    "- Learning Rate: 5e-5\n",
    "- Batch Size: 2\n",
    "- Number of Epochs: 3\n",
    "\n",
    "Here, We are loading the pre-trained Bart model then setting up training arguments for config1, create the custom trainer for config1. Finally, train the model and saving the model weights with full configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 584,
     "referenced_widgets": [
      "49e70fdea6d147ca979dc5d8fc3da1f2",
      "4289a7d9feeb4a4c80b308d2c4e7bd73",
      "ac33d9f7864c4359a28b002709454544",
      "132cc1c035b44b26ba7004cfe567caf2",
      "b2dea6a5163d4d62a6e00d88bc4f1040",
      "2c19dd9ce5384a84b45249b38bc10c93",
      "7ef41c2a23c6461bb45f784ea004250e",
      "727b082218fa464a9b0823c703c1ecab",
      "af26c35b0b014967a69b9a3523f3844f",
      "ac12f48077fd4975afff6639e453ed2a",
      "87abf28d6bed456a834505e6519c60a2",
      "c7c72577af364145a194ff6d9d7feeff",
      "ea8ecc8c9ce54d6d92d2f98d9a5f891c",
      "c48f7253e7e1406789f10ba6017d0774",
      "be9208e7d9ba48eb9c64082891c14ae4",
      "e7438ce0cf954ce5ba8a7b97ff955a7a",
      "bd5baa3eea6b40c3829f82c8a01093c4",
      "0efa179f0a4749d39fa05cae2a4e793c",
      "e8e806e66f3848849c27ae9f22a976b6",
      "d80d630b5da34ef796870d851bd6764a",
      "d03ac3dcfef24ea4946ce0132d2f2ea6",
      "b2fc0b1c39ff4d30bb6cff416dcfa8a1",
      "2d9ff087304649f889a8cdab15d49524",
      "f6f6208caaa74c96912258f2c78d676f",
      "83b51ac2ec844b6a9ec27ef8892394cc",
      "89b6c7e98d8347f2af5f6e873a99e1ba",
      "b3c2a50aaa164ea88af4559870bb7c40",
      "0b1f10ae19f64e94bba8dc5074c2637a",
      "86cc4117f21348b8814e40c5438d5eee",
      "b4af33636a92422f97ac7b3d3f670a1c",
      "29042cd772b24042a788267ec2884cdd",
      "66958bf69e204c0c9318c0ed6ff03c4f",
      "e2a7fc8e5f0346abbdce9ebd86a2dbfa",
      "527f8718faea4a76ade22bf3ccbe2336",
      "95a69047ff724dcdbf7fd4749d00df35",
      "67bfd74e1ffe443c99291e386bc502cc",
      "b53b760bdd714ed69382ac4d1756fd24",
      "1421df08e6a841ee8cd8618989f0c5bb",
      "6f7e33c06c6d4068922293b335376b4f",
      "c379466e1410456b9233acb10a5ffaba",
      "7e75dd62790f4ed492235939bf9617c7",
      "f2aa5722aec2494bbe5218b4ed6f3de6",
      "41662ff8ac4e42b3883c896710c9691e",
      "f434d501a4cc41289ba9b460898192a1",
      "a480354cba18417a93d28b1b4d6f87e1",
      "4e2c49f37b8d416ca37f42b8b79bc429",
      "43104df923234eb3b618adad7a6012c0",
      "9b9a9f37d1f946f18b8f690905c1ce5d",
      "9be1d37fc91a44f381c3a15b498402e5",
      "24eef8c2164d458684298c22216aa12b",
      "532570fbd4a04ff084c053198824c73e",
      "ff0ec83d3d9b4aff8046bf880a4fe363",
      "4b38950ef2d04d8ba4e1f8c0935bacb5",
      "d2da2207454742a79fbb62d39d1361c2",
      "4426ce92f20e44beaa7b58c1bcc0f354",
      "b3d184957be64be98ba5d655e5cfa875",
      "9ee7ed9196cd484bb0a95a57a78f4518",
      "9dd3baee451d41d393e314b346fab9d7",
      "eae5142d0fce4ec082599b192579edda",
      "622a7e7ed63f4bab989a3b4115953408",
      "ac99802f688345c5aeba48221dcb0c42",
      "f0d557655de943fb9260fcdb70945926",
      "4863502f4bbc4fb9a361c1c5994302e8",
      "b4923bea7c38401a81278eb293cb51e6",
      "ca464958e672437c947916fc0f6ba61f",
      "b5052b9a9f894641bbd0e7d5fa45872f"
     ]
    },
    "id": "RUqAKCBbi85M",
    "outputId": "be24ab65-12f2-4bb6-c296-bb8a9a19808c"
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting Experiment: config1\")\n",
    "config1 = experiment_configs[0]\n",
    "model_config1 = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "training_args_config1 = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"billsum_bart_base_{config1['name']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=config1[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config1[\"train_batch_size\"],\n",
    "    learning_rate=config1[\"learning_rate\"],\n",
    "    num_train_epochs=config1[\"num_train_epochs\"],\n",
    "    bf16=True,\n",
    "    logging_dir=f'./logs_{config1[\"name\"]}',\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    report_to=[],\n",
    ")\n",
    "trainer_config1 = CustomSeq2SeqTrainer(\n",
    "    model=model_config1,\n",
    "    args=training_args_config1,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer_config1.train()\n",
    "trainer_config1.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "6B7-2PqVjBKy",
    "outputId": "3a0fb86d-6f2b-44da-c0ca-15f83a3b3a54"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "for log in trainer_config1.state.log_history:\n",
    "    if \"loss\" in log:\n",
    "        train_losses.append(log[\"loss\"])\n",
    "    if \"eval_loss\" in log:\n",
    "        eval_losses.append(log[\"eval_loss\"])\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(len(train_losses), len(train_losses) + len(eval_losses)), eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Logging Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss for config1\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "rBIRpoEMjBwM",
    "outputId": "a2bc7272-8e3d-4376-dc2e-2dd4f9bc046f"
   },
   "outputs": [],
   "source": [
    "eval_results_config1 = trainer_config1.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(f\"Validation results for config1:\")\n",
    "print(eval_results_config1)\n",
    "experiment_results[\"config1\"] = eval_results_config1\n",
    "trained_trainers[\"config1\"] = trainer_config1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration 2 (Config2):\n",
    "- Learning Rate: 3e-5\n",
    "- Batch Size: 2\n",
    "- Number of Epochs: 3\n",
    "\n",
    "Here, We are loading the pre-trained Bart model then setting up training arguments for config1, create the custom trainer for config1. Finally, train the model and saving the model weights with full configuration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "zm_c7l4cjQ8m",
    "outputId": "c13a517e-61cd-4e16-9205-4138dc5bc241"
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting Experiment: config2\")\n",
    "config2 = experiment_configs[1]\n",
    "model_config2 = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "training_args_config2 = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"billsum_bart_base_{config2['name']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=config2[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config2[\"train_batch_size\"],\n",
    "    learning_rate=config2[\"learning_rate\"],\n",
    "    num_train_epochs=config2[\"num_train_epochs\"],\n",
    "    bf16=True,\n",
    "    logging_dir=f'./logs_{config2[\"name\"]}',\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    report_to=[],\n",
    ")\n",
    "trainer_config2 = CustomSeq2SeqTrainer(\n",
    "    model=model_config2,\n",
    "    args=training_args_config2,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer_config2.train()\n",
    "trainer_config2.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "rlcMDf0VjT9z",
    "outputId": "01818c50-62b8-45ce-c732-90d67d97611b"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "for log in trainer_config2.state.log_history:\n",
    "    if \"loss\" in log:\n",
    "        train_losses.append(log[\"loss\"])\n",
    "    if \"eval_loss\" in log:\n",
    "        eval_losses.append(log[\"eval_loss\"])\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(len(train_losses), len(train_losses) + len(eval_losses)), eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Logging Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss for config2\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "Xhj72-VejU0P",
    "outputId": "f8826114-e397-4291-9507-33eca9c0a401"
   },
   "outputs": [],
   "source": [
    "eval_results_config2 = trainer_config2.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(f\"Validation results for config2:\")\n",
    "print(eval_results_config2)\n",
    "experiment_results[\"config2\"] = eval_results_config2\n",
    "trained_trainers[\"config2\"] = trainer_config2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYDCSnXlLwkX",
    "outputId": "72906682-8160-4250-d8ed-c93b66f065ad"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration 3 (Config3):\n",
    "- Learning Rate: 5e-5\n",
    "- Batch Size: 4\n",
    "- Number of Epochs: 3\n",
    "\n",
    "Here, We are loading the pre-trained Bart model then setting up training arguments for config1, create the custom trainer for config1. Finally, train the model and saving the model weights with full configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324
    },
    "id": "tBu79ZgcjZj4",
    "outputId": "aaa81b10-0a06-4834-f9e9-3b607f62d57f"
   },
   "outputs": [],
   "source": [
    "print(\"\\nStarting Experiment: config3\")\n",
    "config3 = experiment_configs[2]\n",
    "model_config3 = BartForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "training_args_config3 = Seq2SeqTrainingArguments(\n",
    "    output_dir=f\"billsum_bart_base_{config3['name']}\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=config3[\"train_batch_size\"],\n",
    "    per_device_eval_batch_size=config3[\"train_batch_size\"],\n",
    "    learning_rate=config3[\"learning_rate\"],\n",
    "    num_train_epochs=config3[\"num_train_epochs\"],\n",
    "    bf16=True,\n",
    "    logging_dir=f'./logs_{config3[\"name\"]}',\n",
    "    logging_steps=50,\n",
    "    predict_with_generate=True,\n",
    "    report_to=[],\n",
    ")\n",
    "trainer_config3 = CustomSeq2SeqTrainer(\n",
    "    model=model_config3,\n",
    "    args=training_args_config3,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer_config3.train()\n",
    "trainer_config3.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "sAzD326bjhTv",
    "outputId": "31a4f128-fb0e-42f1-9528-d1f78f1b4d3a"
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "eval_losses = []\n",
    "for log in trainer_config3.state.log_history:\n",
    "    if \"loss\" in log:\n",
    "        train_losses.append(log[\"loss\"])\n",
    "    if \"eval_loss\" in log:\n",
    "        eval_losses.append(log[\"eval_loss\"])\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(len(train_losses), len(train_losses) + len(eval_losses)), eval_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Logging Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss for config3\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model on the validation set and store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "lXNTEZFBjkYH",
    "outputId": "62880679-1612-4831-ff10-72bc975e05eb"
   },
   "outputs": [],
   "source": [
    "eval_results_config3 = trainer_config3.evaluate(eval_dataset=tokenized_datasets[\"validation\"])\n",
    "print(f\"Validation results for config3:\")\n",
    "print(eval_results_config3)\n",
    "experiment_results[\"config3\"] = eval_results_config3\n",
    "trained_trainers[\"config3\"] = trainer_config3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Methodology**\n",
    "\n",
    "After preprocessing the dataset, we have used a pre-trained model to fine tune on our data. So, started with the pre-trained facebook/bart-base model from Hugging Face, which is known for its strong performance in text generation tasks. \n",
    "\n",
    "Then, we have fine-tuned the model using our tokenized Billsum training data with subsamples. For the evaluation process, we have implemented a custom training by creating a trainer class CustomSeq2SeqTrainer. In this custom trainer, we have override the prediction_step method so that the model’s generate() method is used during evaluation. This will help to generate full summaries using tuned decoding parameters.\n",
    "\n",
    "For decoding, these are some changes:\n",
    "- num_beams=8: thisn is to search over more candidate summaries\n",
    "- length_penalty=1.5: this will encourage the generation of longer, more complete outputs\n",
    "- no_repeat_ngram_size=3: this is to prevent repetition of phrases\n",
    "- min_length=50: and this will ensure that the summaries aren’t too short.\n",
    "\n",
    "We have tried different hyperparameters with different learning rates (5e-5 and 3e-5) and batch sizes (2 and 4) in multiple configurations and finally, we have selected the best-performing configuration based on evaluation metrics specially ROUGE, which is more important here.  For our final evaluation, we have used the best configuration’s trainer to evaluate on a test subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearing GPU Space by removing trainers other than best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWAfdf6qc-8C"
   },
   "outputs": [],
   "source": [
    "for k, t in trained_trainers.items():\n",
    "    if k != \"config3\":\n",
    "        del t\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting config3 as best and then evaluating on test dataset using subset of 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "id": "x9Mt_4iPcM9m",
    "outputId": "d7a9e4d7-c5f3-4606-c30b-6dd2d6659629"
   },
   "outputs": [],
   "source": [
    "best_config_name = \"config3\"\n",
    "print(\"Best configuration based on ROUGE scores:\", best_config_name)\n",
    "best_trainer = trained_trainers[best_config_name]\n",
    "\n",
    "test_metrics = best_trainer.evaluate(eval_dataset=tokenized_datasets[\"test\"].select(range(100)))\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting a bar chart of validation metrics using the best trainer evaluation results of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "HPzZe95Flrj_",
    "outputId": "02b844ab-a7f2-4792-8f53-610319fef9d2"
   },
   "outputs": [],
   "source": [
    "metrics_to_plot = {\n",
    "    \"ROUGE-1\": test_metrics[\"eval_rouge1\"] * 100,\n",
    "    \"ROUGE-2\": test_metrics[\"eval_rouge2\"] * 100,\n",
    "    \"ROUGE-L\": test_metrics[\"eval_rougeL\"] * 100,\n",
    "    \"BLEU\": test_metrics[\"eval_bleu\"],\n",
    "    \"BERTScore\": test_metrics[\"eval_bertscore\"] * 100,\n",
    "}\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(list(metrics_to_plot.keys()), list(metrics_to_plot.values()))\n",
    "plt.title(\"Test Metrics (%) for Best Model\")\n",
    "plt.xlabel(\"Metric\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eR0nYfl4hMW8",
    "outputId": "bd3ca868-b8c1-4d91-fc14-4664c697edc1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqkneB6mhJpO"
   },
   "outputs": [],
   "source": [
    "!cp -r billsum_bart_base_config3 /content/drive/MyDrive/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Analysis and Discussion\n",
    "\n",
    "### Evaluation Scores Analysis\n",
    "\n",
    "**Loss:**  \n",
    "-  Here, The final evaluation shows an evaluation loss of approximately **2.11**.  \n",
    "- This cross-entropy loss value shows that, after 3 epochs on a subset of dataset, the model is learning effectively.  \n",
    "-  Also, a loss value in this range is normal for large sequence-to-sequence models when we are dealing with complex tasks like summarization.\n",
    "\n",
    "**ROUGE Scores:**  \n",
    "1. ROUGE-1: 0.428 (42.8%)  \n",
    "  - This metric is used to measure the overlap of unigrams between the generated summary and the reference. A ROUGE-1 score of 42.8% clearly a strong overlap in the basic content words.  \n",
    "2. ROUGE-2: 0.240 (24.0%)  \n",
    "  - ROUGE-2 will measure the bigram overlap. A score of 24.0% means that the model is able to capture some phrase-level coherence.  \n",
    "3. ROUGE-L: 0.313 (31.3%)  \n",
    "  - ROUGE-L focuses on the longest common subsequence between the generated and reference summaries and it reflects the overall fluency and coherence.\n",
    "\n",
    "These ROUGE metrics shows that the generated summaries closely match the reference summaries in terms of capturing important information.\n",
    "\n",
    "**BLEU Score:**  \n",
    "- The BLEU score is  around 34.41 for config 3. We had higher BLEU in config2 but BLEU is not that useful compared to ROUGE scores as it is more common in machine translation.\n",
    "- Here, we are using sacreBLEU with exponential smoothing and a BLEU score above 30 is good enough. \n",
    "- A high BLEU score here means that the generated summaries have a high degree of n-gram overlap with the reference summaries. But BLEU can be sensitive to minor changes in phrasing.\n",
    "\n",
    "**BERTScore:**  \n",
    "- The BERTScore is around 0.894 or 89.4% .  \n",
    "- BERTScore use contextual embeddings to measure semantic similarity between generated and reference summaries.\n",
    "- A BERTScore close to 0.90 means that the model is able to capture the meaning of the input text very well, even if the exact wording differs.\n",
    "\n",
    "### Loss Graph Analysis\n",
    "\n",
    "- **Training Loss Curve:**  \n",
    "  The plot of training loss over epochs shows a steady decrease, which indicates that the model is learning from the training data consistently.\n",
    "  \n",
    "- **Validation Loss Curve:**  \n",
    "  The validation loss is also decreasing and stays relatively close to the training loss. This suggests that the model is generalizing well to unseen data without significant overfitting.\n",
    "\n",
    "- **Interpretation:**  \n",
    "  - Whenever we see such a smooth convergence of both training and validation losses, it can be considered as a good sign that model is learning.  \n",
    "  - Here the validation loss is not increased significantly relative to the training loss. So, we can say the training is going in good direction.\n",
    "\n",
    "### Challenges Faced\n",
    "\n",
    "1. **Handling Long Documents:**  \n",
    "   - The input text has been shortened to 1024 tokens to save GPU memory and improve processing speed. But, this truncation can might remove some important context, which could lower the quality of the summaries generated.\n",
    "   \n",
    "2. **Variability in Summary Quality:**  \n",
    "   - Summaries naturally vary in wording and depth. Metrics such as BLEU depends on exact phrase matches. So, they can give lower scores due to minor wording differences, even when the meaning are correct.\n",
    "   - By using both ROUGE and BERTScore together, we can achieve a more balanced evaluation. This can capture not just literal phrasing but also deeper semantic similarity.\n",
    "\n",
    "3. **GPU Memory Constraints:**  \n",
    "   - When we are running multiple trainer instances it was consuming lot of GPU memory and all of my 15GB GPU was used while working with google colab.\n",
    "   - Even if we have created subsample as suggested instructions still we had to explicitly delete unused trainer objects (using `del` and `torch.cuda.empty_cache()`) to free up memory for the best model evaluation as otherwise our kernel was crashed once in evalution.\n",
    "\n",
    "4. **Evaluation Speed:**  \n",
    "   - Evaluating the model on the entire test set is very slow.\n",
    "   - So, to get faster feedback, we have evaluated using a smaller subset of the test set (100). This might introduce some variability in results but it significantly decrease evaluation time.\n",
    "\n",
    "### Potential Modifications\n",
    "\n",
    "- **Input/Output Lengths:**  \n",
    "  If we had more GPU resources we can consider to increase the maximum input or target lengths to capture more context from long documents. But this wasn't possible on colab or not practical on ccr.\n",
    "\n",
    "- **Gradient Accumulation:**  \n",
    "  We could implement gradient accumulation to simulate a larger batch size without exceeding GPU memory, which might improve model stability and performance.\n",
    "\n",
    "- **Decoding Parameter Fine-Tuning:**  \n",
    "  We could further try to fine-tune the decoding parameters like adjusting `min_length`, `num_beams`, or `length_penalty` to see if BLEU and other metrics can be improved without decreasing ROUGE or BERTScore.\n",
    "\n",
    "- **Ensemble Methods:**  \n",
    "  We can explore to use ensembles of multiple models to generate summaries, which might increase overall performance and mitigate variability in BLEU scores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- FiscalNote/billsum on Hugging Face: https://huggingface.co/datasets/FiscalNote/billsum\n",
    "- Facebook/bart-base on Hugging Face: https://huggingface.co/facebook/bart-base\n",
    "- Transformers Documentation: https://huggingface.co/docs/transformers/ \n",
    "- Datasets Documentation: https://huggingface.co/docs/datasets/\n",
    "- rouge_score: https://huggingface.co/spaces/evaluate-metric/rouge\n",
    "- sacreBLEU: https://pypi.org/project/sacreBLEU/\n",
    "- BLEU: https://huggingface.co/spaces/evaluate-metric/bleu  \n",
    "- bert_score: https://huggingface.co/spaces/evaluate-metric/bertscore\n",
    "- Python os module Documentation: https://docs.python.org/3/library/os.html\n",
    "- Python random module Documentation: https://docs.python.org/3/library/random.html\n",
    "- NumPy Documentation: https://numpy.org/doc/stable/user/index.html#user\n",
    "- PyTorch Documentation: https://pytorch.org/docs/stable/index.html\n",
    "- Matplotlib Documentation: https://matplotlib.org/stable/users/index.html\n",
    "- Hugging Face Evaluate Documentation: https://huggingface.co/docs/evaluate/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

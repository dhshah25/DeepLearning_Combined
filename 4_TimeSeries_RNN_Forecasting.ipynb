{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KoGJtRJfE_-x",
    "outputId": "aeb9cd7a-d18f-4da1-d0a2-dc619bd2ae7a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TTR04qzMGVDD",
    "outputId": "41f238f2-c0f6-43d7-db3f-720c807bd7b6"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/DL/air+quality.zip\" -d \"/content/airquality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E26ibBNxKBJH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_excel('/content/airquality/AirQualityUCI.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g6NjyK95Kgqx",
    "outputId": "f7241cbd-0502-4215-b886-91630fd45b9e"
   },
   "outputs": [],
   "source": [
    "num_samples = df.shape[0]\n",
    "print(f\"Number of samples (time points): {num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVgjvQuyKmlR",
    "outputId": "d31f715b-28af-47e0-fefe-643e6936afeb"
   },
   "outputs": [],
   "source": [
    "num_features = df.shape[1]\n",
    "print(f\"Number of features: {num_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "N22kx6PbLBTW",
    "outputId": "a497669c-7a4f-4fc2-8d98-58e8492412ca"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whEnguzHLRDM"
   },
   "source": [
    "Dataset Description:\n",
    "The Air Quality dataset contains hourly air pollution measurements collected from an air pollution monitoring station in Italy. It includes data on pollutants such as CO (carbon monoxide), NO2 (nitrogen dioxide), and O3 (ozone), along with temperature and humidity. The dataset originates from the UCI Machine Learning Repository and is commonly used for time series forecasting and environmental analysis.\n",
    "\n",
    "Dataset Source: UCI Air Quality Dataset(https://archive.ics.uci.edu/dataset/360/air+quality)\n",
    "\n",
    "Columns:\n",
    "\n",
    "CO(GT): Concentration of carbon monoxide (mg/m³)\n",
    "NO2(GT): Concentration of nitrogen dioxide (µg/m³)\n",
    "O3(GT): Ozone concentration (µg/m³)\n",
    "Temperature: Ambient temperature (°C)\n",
    "RH: Relative humidity (%)\n",
    "AH: Absolute humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "HDYT82T4LWqy",
    "outputId": "9383a23d-3a40-4fa1-828d-ca90a512d57a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Replace invalid values (-200) with NaN\n",
    "df.replace(-200, np.nan, inplace=True)\n",
    "# Check missing values\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25XGoSIuLyA7",
    "outputId": "2171be2e-96d6-4324-ca67-e7e25c3229b3"
   },
   "outputs": [],
   "source": [
    "df_interpolated = df.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nFmx2sztMjx5",
    "outputId": "dff0a9c8-8ec0-4197-9b55-b47db94bd38a"
   },
   "outputs": [],
   "source": [
    "df_cleaned = df_interpolated\n",
    "print(\"Missing values after imputation:\", df_cleaned.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TM4PT9tfiS0f",
    "outputId": "34aac722-8735-473b-a9d6-2e32bbb2d851"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Assuming 'CO' is your target variable (modify as needed)\n",
    "target_variable = 'CO(GT)'\n",
    "\n",
    "# Visualization 1: Time series plot of target variable\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_cleaned.index, df_cleaned[target_variable])\n",
    "plt.title(f'Time Series of {target_variable}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(target_variable)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Description: This plot shows the temporal variation of CO levels over time, revealing patterns, trends, and potential seasonality.\n",
    "\n",
    "# Visualization 2: Correlation matrix heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "numeric_df = df_cleaned.select_dtypes(include=['number'])\n",
    "correlation = numeric_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Description: This heatmap reveals the correlation between different air quality parameters, helping identify which features may have significant relationships with the target variable.\n",
    "\n",
    "# Visualization 3: Autocorrelation plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "autocorrelation_plot(df_cleaned[target_variable])\n",
    "plt.title(f'Autocorrelation Plot of {target_variable}')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Description: This autocorrelation plot helps identify seasonality and temporal dependencies in the CO measurements, which is crucial for time series forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkSAsA_LiqlJ",
    "outputId": "052fbe1f-d347-46ae-f67e-11cf424008f5"
   },
   "outputs": [],
   "source": [
    "categorical_features = [col for col in df_cleaned.columns if df_cleaned[col].dtype == 'object']\n",
    "if categorical_features:\n",
    "    df_encoded = pd.get_dummies(df_cleaned, columns=categorical_features, drop_first=True)\n",
    "    print(\"Shape after encoding:\", df_encoded.shape)\n",
    "else:\n",
    "    df_encoded = df_cleaned\n",
    "    print(\"No categorical features to encode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPbwwbBNkBZt",
    "outputId": "fed109d1-18ae-4a4c-b7cf-af1d44e784a0"
   },
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(df_encoded))\n",
    "val_size = int(0.15 * len(df_encoded))\n",
    "\n",
    "# Sequential splitting (no shuffling)\n",
    "X_train = df_encoded.iloc[:train_size].drop([target_variable], axis=1)\n",
    "y_train = df_encoded.iloc[:train_size][target_variable]\n",
    "\n",
    "X_val = df_encoded.iloc[train_size:train_size+val_size].drop([target_variable], axis=1)\n",
    "y_val = df_encoded.iloc[train_size:train_size+val_size][target_variable]\n",
    "\n",
    "X_test = df_encoded.iloc[train_size+val_size:].drop([target_variable], axis=1)\n",
    "y_test = df_encoded.iloc[train_size+val_size:][target_variable]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrGlfHE6krkM"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "features = df_cleaned.drop([target_variable], axis=1)\n",
    "\n",
    "datetime_cols = X_train.select_dtypes(include=['datetime64', 'timedelta64']).columns\n",
    "X_train = X_train.drop(datetime_cols, axis=1)\n",
    "X_val = X_val.drop(datetime_cols, axis=1)\n",
    "X_test = X_test.drop(datetime_cols, axis=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "at1HWwjykta-",
    "outputId": "8ff53504-2b45-4abf-ecf8-b9e332dc9b73"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, seq_length):\n",
    "    \"\"\"\n",
    "    Create input sequences and target values for time series forecasting.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        The time series data (should be normalized/scaled).\n",
    "    seq_length : int\n",
    "        The length of input sequences (lookback period).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Input sequences with shape (samples, seq_length, features).\n",
    "    y : numpy.ndarray\n",
    "        Target values with shape (samples,).\n",
    "    \"\"\"\n",
    "    xs = []\n",
    "    ys = []\n",
    "\n",
    "    for i in range(len(data) - seq_length):\n",
    "        x = data[i:(i + seq_length)]\n",
    "        y = data[i + seq_length]\n",
    "\n",
    "        xs.append(x)\n",
    "        ys.append(y)\n",
    "\n",
    "    return np.array(xs), np.array(ys)\n",
    "\n",
    "\n",
    "seq_length = 24  #24 hours (if hourly data)\n",
    "X_train_seq, y_train_seq = create_sequences(np.column_stack((X_train_scaled_df.values, y_train.values.reshape(-1, 1))), seq_length)\n",
    "X_val_seq, y_val_seq = create_sequences(np.column_stack((X_val_scaled_df.values, y_val.values.reshape(-1, 1))), seq_length)\n",
    "X_test_seq, y_test_seq = create_sequences(np.column_stack((X_test_scaled_df.values, y_test.values.reshape(-1, 1))), seq_length)\n",
    "\n",
    "# Extract target variable from the last feature\n",
    "X_train_seq = X_train_seq[:, :, :-1]\n",
    "y_train_seq = y_train_seq[:, -1]\n",
    "\n",
    "X_val_seq = X_val_seq[:, :, :-1]\n",
    "y_val_seq = y_val_seq[:, -1]\n",
    "\n",
    "X_test_seq = X_test_seq[:, :, :-1]\n",
    "y_test_seq = y_test_seq[:, -1]\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(f\"Training sequences shape: {X_train_seq.shape}, Training targets shape: {y_train_seq.shape}\")\n",
    "print(f\"Validation sequences shape: {X_val_seq.shape}, Validation targets shape: {y_val_seq.shape}\")\n",
    "print(f\"Test sequences shape: {X_test_seq.shape}, Test targets shape: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rksk2GiznQj_"
   },
   "outputs": [],
   "source": [
    "#RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9EdGHkTqoVNJ",
    "outputId": "e31cc92c-a08c-4fe0-afab-80b07bdb1c64"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etBAEgYanu48"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters (initial values, will be tuned)\n",
    "input_size = X_train_seq.shape[2]  # Number of features\n",
    "hidden_size = 64\n",
    "num_layers = 3\n",
    "dropout_rate = 0.2\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "seq_length = 24  # Same as used in sequence creation\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_seq)\n",
    "y_train_tensor = torch.FloatTensor(y_train_seq)\n",
    "X_val_tensor = torch.FloatTensor(X_val_seq)\n",
    "y_val_tensor = torch.FloatTensor(y_val_seq)\n",
    "X_test_tensor = torch.FloatTensor(X_test_seq)\n",
    "y_test_tensor = torch.FloatTensor(y_test_seq)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8qQoVj-oRAw"
   },
   "outputs": [],
   "source": [
    "class StackedRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_rate=0.2):\n",
    "        super(StackedRNNModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Stacked RNN layers with tanh activation (default for RNN)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Fully connected layers with ReLU activation\n",
    "        self.fc1 = nn.Linear(hidden_size, 32)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Forward propagate through RNN layers\n",
    "        out, _ = self.rnn(x, h0)\n",
    "\n",
    "        # Get the outputs from the last time step\n",
    "        out = out[:, -1, :]\n",
    "\n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nJ6V-naeodro"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, patience=10):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        train_loss = train_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs')\n",
    "                break\n",
    "\n",
    "    # Load the best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, train_losses, val_losses, best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRFyXEOBogLj"
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tuning():\n",
    "    # Define hyperparameter combinations to try\n",
    "    hidden_sizes = [32, 64, 128]\n",
    "    dropout_rates = [0.1, 0.2, 0.3]\n",
    "    learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for hidden_size in hidden_sizes:\n",
    "        for dropout_rate in dropout_rates:\n",
    "            for lr in learning_rates:\n",
    "                print(f\"\\nTrying: hidden_size={hidden_size}, dropout_rate={dropout_rate}, lr={lr}\")\n",
    "\n",
    "                # Initialize model\n",
    "                model = StackedRNNModel(input_size, hidden_size, num_layers, dropout_rate)\n",
    "\n",
    "                # Initialize optimizer and criterion\n",
    "                criterion = nn.MSELoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "                # Train with early stopping\n",
    "                start_time = time.time()\n",
    "                _, _, _, best_val_loss = train_model(\n",
    "                    model, train_loader, val_loader, criterion, optimizer,\n",
    "                    num_epochs=30, patience=5\n",
    "                )\n",
    "                training_time = time.time() - start_time\n",
    "\n",
    "                # Evaluate on validation set\n",
    "                results.append({\n",
    "                    'hidden_size': hidden_size,\n",
    "                    'dropout_rate': dropout_rate,\n",
    "                    'learning_rate': lr,\n",
    "                    'validation_loss': best_val_loss,\n",
    "                    'training_time': training_time\n",
    "                })\n",
    "\n",
    "                print(f\"Best validation loss: {best_val_loss:.4f}, Training time: {training_time:.2f}s\")\n",
    "\n",
    "    # Find best hyperparameters\n",
    "    results.sort(key=lambda x: x['validation_loss'])\n",
    "    best_params = results[0]\n",
    "\n",
    "    print(\"\\nHyperparameter Tuning Results:\")\n",
    "    for i, res in enumerate(results):\n",
    "        print(f\"{i+1}. hidden_size={res['hidden_size']}, \"\n",
    "              f\"dropout_rate={res['dropout_rate']}, \"\n",
    "              f\"learning_rate={res['learning_rate']}, \"\n",
    "              f\"val_loss={res['validation_loss']:.4f}, \"\n",
    "              f\"time={res['training_time']:.2f}s\")\n",
    "\n",
    "    print(f\"\\nBest hyperparameters: hidden_size={best_params['hidden_size']}, \"\n",
    "          f\"dropout_rate={best_params['dropout_rate']}, \"\n",
    "          f\"learning_rate={best_params['learning_rate']}\")\n",
    "\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEFwNEdToire"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, criterion=torch.nn.MSELoss()):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    batch_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            batch_losses.append(loss.item())  # Store batch-wise loss\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    # Calculate overall metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "\n",
    "    print(f'Test Results:')\n",
    "    print(f'MSE: {mse:.4f}')\n",
    "    print(f'RMSE: {rmse:.4f}')\n",
    "    print(f'MAE: {mae:.4f}')\n",
    "    print(f'R²: {r2:.4f}')\n",
    "\n",
    "    # Plot actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actuals, label='Actual')\n",
    "    plt.plot(predictions, label='Predicted')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot test loss over batches\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(batch_losses, marker='o', linestyle='-', color='b', label='Test Loss (MSE)')\n",
    "    plt.xlabel('Batch Index')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Test Loss Over Batches')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return mse, rmse, mae, r2, predictions, actuals, batch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1t5KbgmUolFm",
    "outputId": "0e39c843-450f-4866-ef86-b2502ad3796d"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Print model summary\n",
    "    sample_input = torch.zeros((batch_size, seq_length, input_size))\n",
    "    model = StackedRNNModel(input_size, hidden_size, num_layers, dropout_rate)\n",
    "    summary(model, input_data=sample_input)\n",
    "    best_params = hyperparameter_tuning()\n",
    "    hidden_size = best_params['hidden_size']\n",
    "    dropout_rate = best_params['dropout_rate']\n",
    "    learning_rate = best_params['learning_rate']\n",
    "\n",
    "    # Initialize model with best parameters\n",
    "    model = StackedRNNModel(input_size, hidden_size, num_layers, dropout_rate)\n",
    "\n",
    "    # Initialize criterion and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    model, train_losses, val_losses, _ = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer,\n",
    "        num_epochs=num_epochs, patience=10\n",
    "    )\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), 'best_rnn_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Aubd47bF8xDu",
    "outputId": "ce53d23c-c656-414b-8c95-41abb68adcbc"
   },
   "outputs": [],
   "source": [
    "mse, rmse, mae, r2, predictions, actuals, batch_losses = evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GJ74-y106US"
   },
   "source": [
    "## Discussion\n",
    "\n",
    "1. Dataset Description\n",
    "The Air Quality dataset from UCI contains 9358 instances of hourly averaged responses from an array of 5 metal oxide chemical sensors embedded in an Air Quality Chemical Multisensor Device1. The device was located in a significantly polluted area at road level within an Italian city, and data were recorded from March 2004 to February 2005. There are 15 features in the dataset.\n",
    "\n",
    "2. RNN Architecture\n",
    "\n",
    "Input Layer - Input size: 35 features\n",
    "\n",
    "RNN Layers- 3 stacked RNN layers\n",
    "\n",
    "64 hidden units per layer\n",
    "\n",
    "Dropout rate of 0.2 between layers\n",
    "\n",
    "Fully Connected Layers: First FC layer: 64 -> 32 units, ReLU activation\n",
    "\n",
    "Second FC layer: 32 -> 1 unit (output)\n",
    "\n",
    "Training Parameters\n",
    "\n",
    "Sequence Length: 24 time steps\n",
    "\n",
    "Batch Size: 32\n",
    "\n",
    "Learning Rate: 0.001\n",
    "\n",
    "Number of Epochs: 100\n",
    "\n",
    "Model Characteristics\n",
    "The model processes sequences of 24 time steps, each with 35 features.\n",
    "\n",
    "It uses a moderate hidden size (64) and number of layers (3), balancing complexity and computational efficiency.\n",
    "\n",
    "The dropout rate of 0.2 helps prevent overfitting. The learning rate of 0.001 is a common starting point for Adam optimizer. With 100 epochs and a batch size of 32, the model has ample opportunity to learn from the data.\n",
    "\n",
    "3. Results\n",
    "\n",
    "Best parameter and the results in the training and validation are : hidden_size=64, dropout_rate=0.2, learning_rate=0.0001, val_loss=0.7952, time=56.09s\n",
    "\n",
    "I performed hyper-parameter tuning on 3 features with 3 possible values assigned for each of them. As a result, there were 27 total combinations. The R2 value is respectable and shows our model has learned decently to make predictions. Adding more hidden layers, decreasing the LR had a relatively negative impact on my model of stacked rnn as my validation loss increased.\n",
    "\n",
    "4. Limitations\n",
    "\n",
    "Stacked RNN architectures have certain limitations. The most obvious ones are, increase in compute time due to the increased complexity. There is a slight potential for overfitting and the need for more data. The UCI Air quality data has only roughly 9400 entries which usually is insufficient to achieve world class performance on complex architectures.\n",
    "\n",
    "5. Future Improvements\n",
    "\n",
    "Try out different architectures, bidirectional RNNs, LSTMs, GRUs on the dataset and also increase the complexity even more. Adding attention mechanisms and trying out ensemble architectures to make the pipeline more robust.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IO5quwhp79r8"
   },
   "source": [
    "STEP 5: REFERENCES\n",
    "\n",
    "- https://www.geeksforgeeks.org/introduction-to-recurrent-neural-network/\n",
    "- https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\n",
    "- https://medium.com/@poudelsushmita878/recurrent-neural-network-rnn-architecture-explained-1d69560541ef\n",
    "- https://machinelearningmastery.com/recurrent-neural-network-algorithms-for-deep-learning/\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

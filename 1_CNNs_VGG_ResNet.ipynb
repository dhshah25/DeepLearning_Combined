{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-apTzmacywV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CD49drR7fYRQ",
    "outputId": "721a05b1-5dd2-46a3-eebd-27e41016e079"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ln1k1YNqfb7l",
    "outputId": "47cf50dd-6b26-4b90-e361-46a5d8b017d1"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/cnn_dataset.zip\" -d \"/content/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TNX-9XwtfnQl",
    "outputId": "b5e8754e-6ecf-4071-9a1d-557a69f2262c"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "data_dir = '/content/cnn_dataset'\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "print(\"Number of images:\", len(dataset))\n",
    "print(\"Classes:\", dataset.classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MbScMlwshY11",
    "outputId": "746d4c31-96ad-462c-d50d-a36361d81883"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class_counts = Counter(label for _, label in dataset.samples)\n",
    "\n",
    "print(\"Number of images per class:\")\n",
    "for class_idx, count in class_counts.items():\n",
    "    class_name = dataset.classes[class_idx]\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing some sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 906
    },
    "id": "oAJ4Z9EDfv7Q",
    "outputId": "85f11ad4-5b21-48c4-b0e1-a1271618ad73"
   },
   "outputs": [],
   "source": [
    "def show_sample_images(dataset, num_samples=3):\n",
    "    fig, axes = plt.subplots(nrows=len(dataset.classes), ncols=num_samples, figsize=(num_samples*3, len(dataset.classes)*3))\n",
    "    for i, class_name in enumerate(dataset.classes):\n",
    "        class_idx = dataset.class_to_idx[class_name]\n",
    "        imgs = [dataset[idx][0].permute(1, 2, 0) for idx in range(len(dataset)) if dataset.imgs[idx][1] == class_idx][:num_samples]\n",
    "        for j, img in enumerate(imgs):\n",
    "            axes[i, j].imshow((img * 0.5 + 0.5).numpy())\n",
    "            axes[i, j].set_title(class_name)\n",
    "            axes[i, j].axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_sample_images(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualizing distribution of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "cUUIZOX-c65b",
    "outputId": "8ffdd00b-b261-4859-d364-5b8099eba18d"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "counts = collections.Counter([label for _, label in dataset.imgs])\n",
    "plt.bar(list(counts.keys()), list(counts.values()))\n",
    "plt.xlabel('Class Index')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Class Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram of pixel distribution of sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "dCouHcrZdA9v",
    "outputId": "eb6bd2b3-5eea-434d-ae0b-bbffb609a168"
   },
   "outputs": [],
   "source": [
    "sample_img = dataset[0][0].numpy().flatten()\n",
    "plt.hist(sample_img, bins=50, color='blue', alpha=0.7)\n",
    "plt.title(\"Histogram of Pixel Values\")\n",
    "plt.xlabel(\"Pixel Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average image of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371
    },
    "id": "D5bX36uJdBbH",
    "outputId": "1d023027-1862-478b-cfe5-9b963359bc77"
   },
   "outputs": [],
   "source": [
    "def compute_average_image(dataset, class_idx):\n",
    "    imgs = [dataset[idx][0].numpy() for idx in range(len(dataset)) if dataset.imgs[idx][1] == class_idx]\n",
    "    return np.mean(np.stack(imgs), axis=0)\n",
    "\n",
    "avg_images = {cls: compute_average_image(dataset, idx) for cls, idx in dataset.class_to_idx.items()}\n",
    "fig, axes = plt.subplots(1, len(avg_images), figsize=(15, 5))\n",
    "for ax, (cls, avg_img) in zip(axes, avg_images.items()):\n",
    "    ax.imshow((avg_img.transpose(1, 2, 0) * 0.5 + 0.5))\n",
    "    ax.set_title(f'Average: {cls}')\n",
    "    ax.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset split and loading splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHP-Zr8ZdFHm"
   },
   "outputs": [],
   "source": [
    "data_indices = list(range(len(dataset)))\n",
    "train_idx, test_idx = train_test_split(data_indices, test_size=0.2, stratify=[s[1] for s in dataset.imgs], random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=[dataset.imgs[i][1] for i in train_idx], random_state=42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oojr1Mtif1i6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "test_dataset  = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sRcYoqkogO0l"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture of VGG16 with some modifications to match dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZOLDvttdGqk"
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "vgg_model = VGG16(num_classes=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Here, this architecture use convulution layer that learns low to high level feature progressively, where starting layers are there to capture patterns like edges and textures and the later layerss will capture more complex shapes and object.\n",
    "\n",
    "2. The MaxPool2d layers will reduce the spatial dimensions, which not only decreases computational load but also helps the learned features to sustain with small translations in the input image.\n",
    "\n",
    "3. The classification section aggregates the extracted features into a compact representation. This is important to distinguish between the three classes by learning complex, non-linear relationships.\n",
    "\n",
    "4. Dropout will help to decrease the level of overfitting. As we are training on 3 classes, each class (dogs, food, vehicles) might have varying amounts of data or some minor intra-class variability.\n",
    "\n",
    "5. Here, I have used one less block than original paper. Additionally, the input is of 64 * 64 size with feature maps using 4 * 4. VGG was designed for 1000 class output but this is modified for 3 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\">Model with kaiming initialization, Adam optimizer and 64 batch size (dropout, learning rate scheduler, regularization and transformations used):</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using kaiming initilaization first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZWSLc9Z1dIiG",
    "outputId": "46bd207b-062f-4b83-8949-4221b6360017"
   },
   "outputs": [],
   "source": [
    "def init_weights(m, init_type='xavier'):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        if init_type == 'xavier':\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        elif init_type == 'he':\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "vgg_model.apply(lambda m: init_weights(m, init_type='he'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adam optimizer with regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWDpts1qdUvU"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer_adam = optim.Adam(vgg_model.parameters(), lr=0.001, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mplFf5E_eq18",
    "outputId": "2390640c-99bc-4aa0-e8c5-2a7365b290b8"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnYXZmGngkUF",
    "outputId": "c36848cc-c945-47ae-8fa5-a076a675ec72"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCel71f6gxx0"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "starting training with learning rate scheduler and wandb to log the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "id": "4XSVpAlWdeQt",
    "outputId": "363e69b4-26e8-48b8-cf5f-6865ff610114"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"vgg16_he_wandb\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_adam, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc_list,  val_acc_list  = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    vgg_model.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train   = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adam.zero_grad()\n",
    "        outputs = vgg_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_adam.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "\n",
    "    vgg_model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg_model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch,\n",
    "        \"Train Loss\": avg_train_loss,\n",
    "        \"Train Accuracy\": train_acc,\n",
    "        \"Val Loss\": avg_val_loss,\n",
    "        \"Val Accuracy\": val_acc,\n",
    "        \"Learning Rate\": optimizer_adam.param_groups[0]['lr']\n",
    "    })\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(vgg_model.state_dict(), 'vgg16_he_best.pth')\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ez9kcVYii9bS"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': vgg_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_adam.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint_vgg16_he.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DfkJ65ClltVj"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading this model for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzFuNkxwl7Zc",
    "outputId": "879e32fb-24c0-45dd-f115-1f7a259d9e02"
   },
   "outputs": [],
   "source": [
    "best_model = VGG16(num_classes=3).to(device)\n",
    "best_model.load_state_dict(torch.load(\"vgg16_he_best.pth\", map_location=device, weights_only=True))\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "GH3MA6nVmEti",
    "outputId": "98097ae7-c5c1-464f-f747-f3da2396ed4e"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mlJ-A6U2mHpk",
    "outputId": "872e4feb-2563-4439-fd96-51ace53e80fa"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\">Model with Xavier initialization, Adam optimizer and 64 batch size (dropout, learning rate scheduler, regularization and transformations used):</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JlpX5pP0mINO"
   },
   "outputs": [],
   "source": [
    "vgg_model_xavier = VGG16(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gSOSSTmemQdn",
    "outputId": "0518c146-a1c5-408f-f615-6312d5851753"
   },
   "outputs": [],
   "source": [
    "vgg_model_xavier.apply(lambda m: init_weights(m, init_type='xavier'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjSZaeTDmgZ8"
   },
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(vgg_model_xavier.parameters(), lr=0.001, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NX-vNe94mjKf",
    "outputId": "d85b03fc-4ccc-4d54-c3a7-3ba5056fbae5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model_xavier.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v9SPnjDEmpFZ",
    "outputId": "789333db-73c6-4baa-97ca-b1267c43db63"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_adam, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc_list,  val_acc_list  = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    vgg_model_xavier.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train   = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adam.zero_grad()\n",
    "        outputs = vgg_model_xavier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_adam.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    vgg_model_xavier.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg_model_xavier(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(vgg_model_xavier.state_dict(), 'vgg16_xavier_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving logs to wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "ydNfLcH6qgPW",
    "outputId": "e9d7c7aa-b9a5-48ca-9857-7bb9e2b9b06c"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"vgg16_xavier_wandb\")\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss_list[epoch],\n",
    "        \"Train Accuracy\": train_acc_list[epoch],\n",
    "        \"Val Loss\": val_loss_list[epoch],\n",
    "        \"Val Accuracy\": val_acc_list[epoch]\n",
    "    })\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7_t_8Bpq1oS"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': vgg_model_xavier.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_adam.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint_vgg16_xavier.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WcaELqhPrBuj",
    "outputId": "c72e0421-64ac-4e5c-d931-28383c2ea3f0"
   },
   "outputs": [],
   "source": [
    "best_model = VGG16(num_classes=3).to(device)\n",
    "best_model.load_state_dict(torch.load(\"vgg16_xavier_best.pth\", map_location=device, weights_only=True))\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "xuTI3pscrZ2e",
    "outputId": "249c30ae-72de-476b-c07b-1253c75c4cf5"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3A3_FtTsrdYB",
    "outputId": "5c3e919c-514c-4867-9b5f-50aff981de74"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaiming initilization is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\">Model with kaiming initialization, AdamW optimizer and 64 batch size (dropout, learning rate scheduler, regularization and transformations used):</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwwZUuSArlwd"
   },
   "outputs": [],
   "source": [
    "vgg_model_adamw = VGG16(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bak5XWsGrnlX",
    "outputId": "15146452-4ff5-4c4b-ff33-dd459986edbc"
   },
   "outputs": [],
   "source": [
    "vgg_model_adamw.apply(lambda m: init_weights(m, init_type='he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pxvMwLCrwLH"
   },
   "outputs": [],
   "source": [
    "optimizer_adamw = optim.AdamW(vgg_model_adamw.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HNQUQCPysTM-",
    "outputId": "5d0a7e19-fae1-4e60-92c9-2ee4587adadf"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model_adamw.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFLa9kPksaJO",
    "outputId": "c8f94d29-97ae-40d4-a574-ee739df87d4d"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_adamw, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc_list,  val_acc_list  = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    vgg_model_adamw.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train   = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adamw.zero_grad()\n",
    "        outputs = vgg_model_adamw(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_adamw.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    vgg_model_adamw.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg_model_adamw(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(vgg_model_adamw.state_dict(), 'vgg16_adamw_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High loss on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "A_hpm5PitAdA",
    "outputId": "c0c013ff-ba60-4bab-cab0-e99fb957b541"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"vgg16_adamw_wandb\")\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss_list[epoch],\n",
    "        \"Train Accuracy\": train_acc_list[epoch],\n",
    "        \"Val Loss\": val_loss_list[epoch],\n",
    "        \"Val Accuracy\": val_acc_list[epoch]\n",
    "    })\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPsLeYm3waeS"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': vgg_model_adamw.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_adamw.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint_vgg16_adamw.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZStVLM9wsrp",
    "outputId": "5a799f48-5f10-422e-cece-7620f0e50fe5"
   },
   "outputs": [],
   "source": [
    "best_model = VGG16(num_classes=3).to(device)\n",
    "best_model.load_state_dict(torch.load(\"vgg16_adamw_best.pth\", map_location=device, weights_only=True))\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "QzwOLuqiw2Tk",
    "outputId": "028749c0-12fb-47b2-c043-8aeb93d73d8c"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LQ8I4lnw88j",
    "outputId": "faebc1a7-5a20-4d15-d499-78de89f5dfaa"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\">Model with kaiming initialization, SGD optimizer and 64 batch size (dropout, learning rate scheduler, regularization and transformations used):</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJ3_Yu3mw-Bi"
   },
   "outputs": [],
   "source": [
    "vgg_model_sgd = VGG16(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R381glG4xL6X",
    "outputId": "00b2629b-f63e-43ed-b229-3b44e33dd7b2"
   },
   "outputs": [],
   "source": [
    "vgg_model_sgd.apply(lambda m: init_weights(m, init_type='he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDyO5Pg4xI8x"
   },
   "outputs": [],
   "source": [
    "optimizer_sgd = optim.SGD(vgg_model_sgd.parameters(), lr=0.01, momentum=0.9, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ac8U74MGxTbU",
    "outputId": "7b677b8b-e4b4-4301-c527-dc8b330916fb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model_sgd.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WVhCi89hxbcW",
    "outputId": "0a3030e6-c181-49b2-b235-9df56de720dc"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_sgd, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc_list,  val_acc_list  = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    vgg_model_sgd.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train   = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_sgd.zero_grad()\n",
    "        outputs = vgg_model_sgd(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_sgd.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    vgg_model_sgd.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg_model_sgd(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(vgg_model_sgd.state_dict(), 'vgg16_sgd_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "0M-GhJkAyb8s",
    "outputId": "2f946af9-311d-4211-b90b-59dca7ae1ab1"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"vgg16_sgd_wandb\")\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss_list[epoch],\n",
    "        \"Train Accuracy\": train_acc_list[epoch],\n",
    "        \"Val Loss\": val_loss_list[epoch],\n",
    "        \"Val Accuracy\": val_acc_list[epoch]\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nnqvCG_Cxx-w"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': vgg_model_sgd.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_sgd.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint_vgg16_sgd.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation on tesing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mOxkdmeryZFn",
    "outputId": "86bd1c9d-3eed-4ae4-cc75-7d0349567054"
   },
   "outputs": [],
   "source": [
    "best_model = VGG16(num_classes=3).to(device)\n",
    "best_model.load_state_dict(torch.load(\"vgg16_sgd_best.pth\", map_location=device, weights_only=True))\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "agSkHnd_yoKu",
    "outputId": "22654408-204d-4382-ec47-8ac7c02a7810"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O5MliKCJyr29",
    "outputId": "1c7529cd-89b8-4fb4-c417-85707c9c2ae2"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision: <br>\n",
    "Model with kaiming initilization, adam optimizer and batch size 64 including some optimization is performing better. Although using AdamW instead of Adam with same configuration is giving slighly high accuracy of *94.32%* compared to *93.67%* from previous configuration. The loss on test is *0.2284* for AdamW and *0.2183* for previous model. Additionally, the validation loss is high on model with AdamW. So we will proceed with (kaiming initilization, adam optimizer and batch size 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\">Model with kaiming initialization, Adam optimizer and 128 batch size (same optimizations):</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kapK5uYF4_KR"
   },
   "outputs": [],
   "source": [
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "test_dataset  = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=128, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwxmrFEzzXYX"
   },
   "outputs": [],
   "source": [
    "vgg_model_batch = VGG16(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XTmc7sTX2NDC",
    "outputId": "c100dc25-9a70-457f-ccaf-07a3d11f73c7"
   },
   "outputs": [],
   "source": [
    "vgg_model_batch.apply(lambda m: init_weights(m, init_type='he'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQqwwdEl2fVf"
   },
   "outputs": [],
   "source": [
    "optimizer_adam = optim.Adam(vgg_model_batch.parameters(), lr=0.001, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z6OQDAJj2qEV",
    "outputId": "01a352c9-c552-489d-cad7-3d89a0f91bbb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vgg_model_batch.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPVqzXa32wcn",
    "outputId": "7d5e5e29-f0e0-4780-d05a-a97aafaf37ac"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_adam, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "num_epochs = 20\n",
    "best_val_acc = 0\n",
    "\n",
    "train_loss_list, val_loss_list = [], []\n",
    "train_acc_list,  val_acc_list  = [], []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "    vgg_model_batch.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train   = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer_adam.zero_grad()\n",
    "        outputs = vgg_model_batch(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer_adam.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    train_acc = 100. * correct_train / total_train\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "\n",
    "    vgg_model_batch.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val   = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = vgg_model_batch(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_acc = 100. * correct_val / total_val\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "          f\"Train Acc: {train_acc:.2f}% | Train Loss: {avg_train_loss:.4f} | \"\n",
    "          f\"Val Acc: {val_acc:.2f}% | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(vgg_model_batch.state_dict(), 'vgg16_batch_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "pxXNmqCa36I4",
    "outputId": "77b9fc52-d839-4e8f-e5fd-f736e2e60bc3"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"vgg16_batch_wandb\")\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss_list[epoch],\n",
    "        \"Train Accuracy\": train_acc_list[epoch],\n",
    "        \"Val Loss\": val_loss_list[epoch],\n",
    "        \"Val Accuracy\": val_acc_list[epoch]\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teN2BjPF6AFZ"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': num_epochs,\n",
    "    'model_state_dict': vgg_model_batch.state_dict(),\n",
    "    'optimizer_state_dict': optimizer_adam.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "torch.save(checkpoint, 'checkpoint_vgg16_batch.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hddu6yE66LM3",
    "outputId": "556ab230-06c0-4081-80c6-33d4477cd216"
   },
   "outputs": [],
   "source": [
    "best_model = VGG16(num_classes=3).to(device)\n",
    "best_model.load_state_dict(torch.load(\"vgg16_batch_best.pth\", map_location=device, weights_only=True))\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = best_model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "lMaSH4pd6U8C",
    "outputId": "dd46730c-939a-48f4-cc4b-2b7fcbf19c79"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQiEqPq06bXz",
    "outputId": "9e9ea006-2524-42b2-fd02-5c1dc02aa773"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision: <br>\n",
    "After trying different initilization, optimizers and batch size with multiple optimization our best choice is kaiming initilization, Adam optimizer, batch size 64 and optimizations like dropout, tranformations, regularization and learning rate scheduler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the model for full visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6lmDGLdz8qJK",
    "outputId": "dac4ec70-c848-49ed-b010-c3e4044f8f15"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VGG16(num_classes=3).to(device)\n",
    "\n",
    "checkpoint = torch.load(\"checkpoint_vgg16_he.pth\", map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "train_loss_history = checkpoint['train_loss_history']\n",
    "val_loss_history   = checkpoint['val_loss_history']\n",
    "train_acc_history  = checkpoint['train_acc_history']\n",
    "val_acc_history    = checkpoint['val_acc_history']\n",
    "\n",
    "best_val_acc = checkpoint.get('best_val_acc', 0)\n",
    "print(f\"Checkpoint loaded. Best Val Accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "22VjUDPX9tv6",
    "outputId": "3242feeb-3574-4038-fef5-7f1f8cd0fbc9"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(train_acc_history) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, train_acc_history, 'b-', label='Train Accuracy')\n",
    "plt.plot(epochs, val_acc_history,   'r-', label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (fraction)')\n",
    "plt.title('Training vs. Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "GcPp0Xtc9uZk",
    "outputId": "b312de44-004e-477e-c406-5cf8b8cd6aa0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(epochs, train_loss_history, 'b-', label='Train Loss')\n",
    "plt.plot(epochs, val_loss_history,   'r-', label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpSzk5_090dm",
    "outputId": "cc55a499-375e-4987-f40f-54e335086f3d"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "\n",
    "        _, predicted = outputs.max(dim=1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss /= total\n",
    "test_acc = 100. * correct / total\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "NqEnDDFo96Si",
    "outputId": "0ab801a0-b2ac-4868-ad9d-ee70e93663f3"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=dataset.classes, yticklabels=dataset.classes)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - VGG16\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l4NyGaKQ98Di",
    "outputId": "58a07b3c-bce5-4ac7-890b-a3154f3c4054"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1 Score:  {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing misclassified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 769
    },
    "id": "Qo_qpGlCHJHD",
    "outputId": "27495ade-af5b-4d08-a3a4-d3e8e766d7d1"
   },
   "outputs": [],
   "source": [
    "misclassified_images = []\n",
    "misclassified_preds = []\n",
    "misclassified_labels = []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            if predicted[i] != labels[i]:\n",
    "                misclassified_images.append(images[i].cpu())\n",
    "                misclassified_preds.append(predicted[i].cpu().item())\n",
    "                misclassified_labels.append(labels[i].cpu().item())\n",
    "            if len(misclassified_images) >= 6:\n",
    "                break\n",
    "        if len(misclassified_images) >= 6:\n",
    "            break\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12,8))\n",
    "axes = axes.flatten()\n",
    "for i, ax in enumerate(axes):\n",
    "    img = misclassified_images[i].permute(1, 2, 0).numpy()\n",
    "    ax.imshow(img.clip(0,1))\n",
    "    ax.set_title(f\"True: {dataset.classes[misclassified_labels[i]]}\\n\"\n",
    "                 f\"Pred: {dataset.classes[misclassified_preds[i]]}\")\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:salmon; font-size:20px;\"> ResNet 18</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxGYkZo7KfDD"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE   = 64\n",
    "LR           = 0.001\n",
    "EPOCHS       = 20\n",
    "NUM_CLASSES  = 3\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "dataset_dir = \"/content/cnn_dataset\"\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=dataset_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WPZHxmDwLLYF"
   },
   "outputs": [],
   "source": [
    "train_indices, test_indices = train_test_split(\n",
    "    range(len(full_dataset)),\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=[sample[1] for sample in full_dataset.samples]\n",
    ")\n",
    "\n",
    "train_indices, val_indices = train_test_split(\n",
    "    train_indices,\n",
    "    test_size=0.1765,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=[full_dataset.samples[i][1] for i in train_indices]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joWu7tGpLOia"
   },
   "outputs": [],
   "source": [
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset   = Subset(full_dataset, val_indices)\n",
    "test_dataset  = Subset(full_dataset, test_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jy39Je4gLPwB",
    "outputId": "1f836ff3-4faf-44d3-8d6d-75e44d729d84"
   },
   "outputs": [],
   "source": [
    "print(f\"Train set: {len(train_dataset)} images\")\n",
    "print(f\"Val set:   {len(val_dataset)} images\")\n",
    "print(f\"Test set:  {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hra3zS8oK5w9"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(out_channels)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool = nn.Identity()\n",
    "\n",
    "        self.layer1 = self._make_layer(64,  2, stride=1)\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FZRUODEXLBg7",
    "outputId": "265c69a0-56e9-4267-a704-5d2e0a2c1ef7"
   },
   "outputs": [],
   "source": [
    "model_resnet = ResNet18(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_resnet.parameters(), lr=LR, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "\n",
    "train_loss_list = []\n",
    "val_loss_list   = []\n",
    "train_acc_list  = []\n",
    "val_acc_list    = []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_resnet.train()\n",
    "    running_train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += predicted.eq(labels).sum().item()\n",
    "\n",
    "    avg_train_loss = running_train_loss / len(train_loader)\n",
    "    train_acc = 100.0 * correct_train / total_train\n",
    "\n",
    "    model_resnet.eval()\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model_resnet(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    avg_val_loss = running_val_loss / len(val_loader)\n",
    "    val_acc = 100.0 * correct_val / total_val\n",
    "\n",
    "    train_loss_list.append(avg_train_loss)\n",
    "    val_loss_list.append(avg_val_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}] \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model_resnet.state_dict(), \"resnet18_best.pth\")\n",
    "\n",
    "print(f\"Training Complete. Best Validation Accuracy: {best_val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EibymnRMNX3C"
   },
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    'epoch': EPOCHS,\n",
    "    'model_state_dict': model_resnet.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    'train_loss_history': train_loss_list,\n",
    "    'val_loss_history': val_loss_list,\n",
    "    'train_acc_history': train_acc_list,\n",
    "    'val_acc_history': val_acc_list,\n",
    "    'best_val_acc': best_val_acc\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBaVI-CoLHER",
    "outputId": "174abbf7-7290-43ba-9721-d2d7e833e106"
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('checkpoint_resnet18.pth', map_location='cpu')\n",
    "\n",
    "train_loss_history = checkpoint['train_loss_history']\n",
    "val_loss_history   = checkpoint['val_loss_history']\n",
    "train_acc_history  = checkpoint['train_acc_history']\n",
    "val_acc_history    = checkpoint['val_acc_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "9EYTtGxVN9H9",
    "outputId": "9fb80d78-fe8f-4f76-ba42-ffebffa31037"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('ResNet-18 Training/Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "qhO7KzXDN-YJ",
    "outputId": "855d35fb-90cd-4739-91f9-d50d2569ba26"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(train_acc_history, label='Train Accuracy')\n",
    "plt.plot(val_acc_history, label='Val Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('ResNet-18 Training/Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "THNus2AzQtOE",
    "outputId": "becccb92-66d1-4062-be28-56484f4652a7"
   },
   "outputs": [],
   "source": [
    "model_resnet = ResNet18(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "model_resnet.load_state_dict(torch.load(\"resnet18_best.pth\"))\n",
    "model_resnet.eval()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model_resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += predicted.eq(labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_acc = 100.0 * correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {avg_test_loss:.4f} | Test Accuracy: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "UjkWyr_JSb5J",
    "outputId": "5f0e2333-fe1e-43f0-bcee-fd92195c95c6"
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=full_dataset.classes,\n",
    "            yticklabels=full_dataset.classes)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"ResNet-18 Confusion Matrix (Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bGma-xNeSeEu",
    "outputId": "3a37fee9-4baa-4327-d0d7-45101b441c1d"
   },
   "outputs": [],
   "source": [
    "precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='weighted')\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSZTA11FSgd0"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "showing misclassified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 226
    },
    "id": "jsM8Xc_-Sf2h",
    "outputId": "33bf19b5-2734-42e1-9267-730f7f0bdd68"
   },
   "outputs": [],
   "source": [
    "misclassified_indices = [i for i, (true_lbl, pred_lbl) in enumerate(zip(all_labels, all_preds))\n",
    "                         if true_lbl != pred_lbl]\n",
    "random.shuffle(misclassified_indices)\n",
    "\n",
    "num_to_show = 5\n",
    "sample_misclassified = misclassified_indices[:num_to_show]\n",
    "\n",
    "fig, axes = plt.subplots(1, num_to_show, figsize=(5*num_to_show, 4))\n",
    "\n",
    "for ax, idx in zip(axes, sample_misclassified):\n",
    "\n",
    "    img, true_label = test_dataset[idx]\n",
    "\n",
    "\n",
    "    img_np = img.permute(1, 2, 0).numpy()\n",
    "    img_np = (img_np * 0.5) + 0.5\n",
    "    img_np = np.clip(img_np, 0, 1)\n",
    "\n",
    "    ax.imshow(img_np)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"Pred: {full_dataset.classes[all_preds[idx]]}\\nTrue: {full_dataset.classes[true_label]}\")\n",
    "\n",
    "plt.suptitle(\"Misclassified Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 567
    },
    "id": "wGa7YFGTTEyp",
    "outputId": "3300d67a-7b74-460c-d4f3-300015e6f6cf"
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"my_vgg16_project\", name=\"ResNet_wandb\")\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    wandb.log({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        \"Train Loss\": train_loss_list[epoch],\n",
    "        \"Train Accuracy\": train_acc_list[epoch],\n",
    "        \"Val Loss\": val_loss_list[epoch],\n",
    "        \"Val Accuracy\": val_acc_list[epoch]\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](https://lh3.googleusercontent.com/d/1I-n8AlLzQga3Jk_4_BfcvHXZYEIsLmsV=w1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dXlKv7PcUI64"
   },
   "outputs": [],
   "source": [
    "checkpoint_vgg = torch.load(\"checkpoint_vgg16_he.pth\", map_location='cpu', weights_only=True)\n",
    "train_loss_vgg = checkpoint_vgg['train_loss_history']\n",
    "val_loss_vgg   = checkpoint_vgg['val_loss_history']\n",
    "train_acc_vgg  = checkpoint_vgg['train_acc_history']\n",
    "val_acc_vgg    = checkpoint_vgg['val_acc_history']\n",
    "\n",
    "checkpoint_resnet = torch.load(\"checkpoint_resnet18.pth\", map_location='cpu', weights_only=True)\n",
    "train_loss_resnet = checkpoint_resnet['train_loss_history']\n",
    "val_loss_resnet   = checkpoint_resnet['val_loss_history']\n",
    "train_acc_resnet  = checkpoint_resnet['train_acc_history']\n",
    "val_acc_resnet    = checkpoint_resnet['val_acc_history']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "lj-dmH8tUaW5",
    "outputId": "9d73b01d-1e44-4a26-e61d-9f15c78ceb94"
   },
   "outputs": [],
   "source": [
    "epochs_vgg = range(1, len(train_loss_vgg) + 1)\n",
    "epochs_resnet = range(1, len(train_loss_resnet) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs_vgg, train_acc_vgg, 'b-', label='VGG-16 Train Acc')\n",
    "plt.plot(epochs_vgg, val_acc_vgg,   'b--', label='VGG-16 Val Acc')\n",
    "plt.plot(epochs_resnet, train_acc_resnet, 'r-', label='ResNet-18 Train Acc')\n",
    "plt.plot(epochs_resnet, val_acc_resnet,   'r--', label='ResNet-18 Val Acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "i7Ao4TDiUy7f",
    "outputId": "67e83ade-adc2-4368-f429-a0ad5e101a30"
   },
   "outputs": [],
   "source": [
    "plt.plot(epochs_vgg, train_loss_vgg, 'b-', label='VGG-16 Train Loss')\n",
    "plt.plot(epochs_vgg, val_loss_vgg,   'b--', label='VGG-16 Val Loss')\n",
    "plt.plot(epochs_resnet, train_loss_resnet, 'r-', label='ResNet-18 Train Loss')\n",
    "plt.plot(epochs_resnet, val_loss_resnet,   'r--', label='ResNet-18 Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Decision** <br>\n",
    "\n",
    "ResNet is better option with less loss and accuracy. Although VGG with Adam or AdamW has slightly high accuracy this accuracy is not that bad as the difference is minor. The opther important part is the loss on testing is low as well (0.16). So, we are considering this as final best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theoretical Concepts**\n",
    "\n",
    "1. VGG\n",
    "    - The VGG paper shows that stacking many small 3 * 3 filters instead of larger kernels like 5 * 5 or 7 * 7 can achieve very deep networks and also keep the number of parameters relatively manageable.\n",
    "    - A single 3 * 3 convolution layer has fewer parameters than a 5 * 5 or 7 * 7, and stacking multiple 3 * 3 layers in sequence increases the effective receptive field and yet maintains the fewer parameters per layer. This design makes the network easier to train and improves generalization.\n",
    "    - VGG architectures have multiple convolution blocks, each followed by max-pooling. As we go deeper, the network learns progressively more abstract features like from edges -> textures -> parts -> objects.\n",
    "    - The deeper the network, the richer the representational capacity but very deep networks can suffer from computational costs and might leads to overfitting if not properly regularized.\n",
    "\n",
    "2. ResNet\n",
    "    - When networks grow very deep, gradients must propagate back through many layers. This can lead to extremely small which is vanishing or extremely large which is exploding gradient values. Here, vanishing gradients slow or halt learning in early layers and exploding gradients might cause numerical instability.\n",
    "    - Now, the ResNet introduced the concept of skip connections where the input to a set of layers is added directly to the output of those layers. This residual connection will make it easier for the network to learn an identity mapping if it's optimal. Here, gradients can flow directly through the skip path and alleviate the vanishing gradient issue and allow the networks to be much deeper without a much loss in trainability.\n",
    "    - Identity mapping is the idea that if a certain set of layers (the residual block) does not need to transform the input significantly, it can just learn to output zero so that the blocks output is effectively the same as its input. So, this flexibility significantly stabilizes and speeds up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularization and Optimization Techniques**\n",
    "\n",
    "1. Data Augmentation:\n",
    "    - It reduces overfitting by increasing the effective size and variability of the dataset. This might led to higher accuracy on the test/validation set.\n",
    "    - VGG and ResNet both benefit equally in principle, but deeper networks may use augmented data even more effectively.\n",
    "    - VGG has a large number of parameters in its fully connected layers, so it will make it prone to overfitting if the dataset is not huge. Data augmentation helps the network generalize better. ResNet handle overfitting slightly better due to the skip connections helps for smoother gradient flow and also requires fewer parameters overall.\n",
    "\n",
    "2. Dropout:\n",
    "    - It helps to prevent overfitting by randomly zeroing out neurons. In VGG, dropout is generally placed after the final pooling or in the fully connected layers. But in ResNet, its less frequently used in standard blocks.\n",
    "    - Without droput it is possible that model may overfit on training data and perform slighly less on validation with slighly higher loss. But by using dropout this issue can be avoided. In ResNet it might shom minor improvement if dropout used.\n",
    "\n",
    "3. Weight Decay (Regularization):\n",
    "    - Here, the penalty on the size of the weights, controlled by a regularization parameter. VGG is quite deep and has large fully connected layers, so weight decay is crucial to prevent overfitting.\n",
    "    - ResNet also benefits from weight decay but it may be does not make much difference due to to fewer total parameters than VGG-16.\n",
    "\n",
    "4. Learning Rate Scheduler:\n",
    "    - By using ReduceLROnPlateau, it is possible to minimize or maximize metric. If the monitored metric does not improve for patience epochs, the scheduler will reduce the learning rate by a factor. If the network is still improving, the LR remains the same. This helps you continue making fast progress.\n",
    "    - Large learning rates can cause the model to overshoot or oscillate around a local optimum. Reducing the LR at the right time can lead to better convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyzing**\n",
    "\n",
    "- VGG-16 may show a slower convergence initially but it can reach a high training accuracy near the end. But in case of ResNet-18 it converges faster, with a steeper climb in training accuracy during the early epochs. Generally, ResNets skip connections help gradients flow more easily so it will speed up the training.\n",
    "- Loss is kind of less and similer for both training and validation in ResNet but with VGG even if validation loss is less the accuracy on training is higher compared to validation accuracy this suggest the possiblity of overfitting.\n",
    "- ResNet is showing great accuracy on testing dataset. Even if VGG has slighly higher accurracy that difference is much less compared to loss of both model on testing dataset, where ResNet shows much less loss than VGG on testing as well. Additionally, the percent of loss is mostly same amongst training, validation and testing dataset and same for accuracy in  case of Resnet. So, we are considering ResNet as our final best model.\n",
    "- VGG sometimes misclassifies images with busy backgrounds or low contrast. But ResNet might handle complex textures better due to skip connections that preserves gradient flow in deeper layers.\n",
    "- In most cases ResNet-18 outperforms VGG-16, especially on moderate to large datasets and the reason is the residual connections which makes training deeper networks easier. VGG-16 is older and has more parameters in the classifier section which might lead to overfitting if the dataset is not large enough. But ResNet-18 has skip connections, which helps it to learn faster and often produce higher validation accuracy with fewer training epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "- VGG stacks many 3 * 3 convolutions in succession without skip connections and relies on a large number of parameters in its final fully connected layers. But ResNet uses residual (skip) connections, which enables deeper networks to train more easily by allowing gradients to flow through identity paths.\n",
    "-  VGG might converge more slowly and might shows a larger gap between training and validation accuracy if the dataset is not large enough. It can also be more prone to overfitting without sufficient regularization. But ResNet converges faster and achieves a better or decent final accuracy and less loss on testing with fewer parameters due to the residual connections that alleviate the vanishing gradient problem\n",
    "- In ResNet, the skip connections allow gradients to bypass certain layers, which mitigates vanishing gradients in deeper networks. So ResNet might require less epochs to reach the best performance. Also the validation and testing accuracy is very good with very less loss on both.\n",
    "- So, based on all the analysis and experiments we think that ResNet is the final best model for this task with less overfitting, more generalization and more computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:** <br>\n",
    "\n",
    "https://matplotlib.org/stable/plot_types/index.html <br>\n",
    "https://numpy.org/doc/stable/user/index.html#user <br>\n",
    "https://seaborn.pydata.org/tutorial/introduction.html <br>\n",
    "https://pytorch.org/docs/stable/index.html <br>\n",
    "https://scikit-learn.org/stable/ <br>\n",
    "https://arxiv.org/pdf/1512.03385 <br>\n",
    "https://docs.wandb.ai/guides/track/ <br>\n",
    "https://arxiv.org/abs/1409.1556 <br>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

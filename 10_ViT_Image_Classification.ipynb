{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7yk813FA2qT",
    "outputId": "ff5faf63-3ed6-4379-c4b1-db1e6cd7646f"
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.25.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nApM69sDdUoA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from google.colab import drive\n",
    "import os\n",
    "from PIL import Image, UnidentifiedImageError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preparation & Cleaning:**\n",
    "- Here, it includes unzipping the dataset and removing corrupted images.\n",
    "- We have applied data augmentation (random horizontal flips) for training and normalization for both training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47WOK8d2D0zj",
    "outputId": "51970960-d929-448d-c807-725f6079ba44"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "zip_path = \"/content/drive/MyDrive/catsanddogs.zip\"\n",
    "extract_dir = \"./\"\n",
    "\n",
    "if not os.path.exists(\"./PetImages\"):\n",
    "    !unzip -q \"{zip_path}\" -d \"{extract_dir}\"\n",
    "    print(\"Dataset unzipped.\")\n",
    "else:\n",
    "    print(\"PetImages folder already exists, skipping unzip.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xnxrc23DdJSu",
    "outputId": "5e6e8dcc-76bd-4037-8444-b1877ef3fd75"
   },
   "outputs": [],
   "source": [
    "def remove_corrupted_images(directory):\n",
    "    removed_count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            try:\n",
    "                with Image.open(file_path) as img:\n",
    "                    img.verify()\n",
    "            except (UnidentifiedImageError, IOError, SyntaxError) as e:\n",
    "                print(f\"Removing corrupted file: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                removed_count += 1\n",
    "    print(f\"Removed {removed_count} corrupted image files.\")\n",
    "remove_corrupted_images(\"./kagglecatsanddogs_5340/PetImages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J7VTvRcYdb5F",
    "outputId": "00a5cff4-53c9-4f01-d7bd-fdd14f08b5f4"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZ24jfFsddr_"
   },
   "outputs": [],
   "source": [
    "data_dir = \"./kagglecatsanddogs_5340/PetImages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6YcBZQpZdf8_"
   },
   "outputs": [],
   "source": [
    "def is_valid_image(filename):\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "    return filename.lower().endswith(valid_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205,
     "referenced_widgets": [
      "cae126ffe4f549e3b09b88ca40cd9e8b",
      "0166613f4a2e4ebc99a08f64b773993b",
      "9066d37077b24a88a41428eb1c01353e",
      "84f14d700a354555b6b89a2f34beb43b",
      "941660744c6d4488a07a2e3b5e5c1d61",
      "c46c220ab77b4785be6c2582b62a4b81",
      "4bea9d183c984763bfe03205742fa200",
      "4170effebd734e3f8f3df84a1447356c",
      "beaa32122fdb46afaa8a8c780a27de8a",
      "a68278a14257499ab53a4285c063b21c",
      "4dd997e85c62476cbb504d7acdeb3b25"
     ]
    },
    "id": "b_Brsl_GdhkG",
    "outputId": "80f92093-6aeb-4713-f681-1e4de0cfe1dc"
   },
   "outputs": [],
   "source": [
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHGgFNu8djds"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jL_I2dcdrSh",
    "outputId": "0c7f28b8-5533-4cf3-a552-7c768c331367"
   },
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(\n",
    "    root=data_dir,\n",
    "    transform=train_transform,\n",
    "    is_valid_file=is_valid_image\n",
    ")\n",
    "print(\"Total images found:\", len(full_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Splitting:**\n",
    "- The data is divided into 70% training, 15% validation, and 15% test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v07E-3JEiCYo",
    "outputId": "3fbe37e6-03ee-4531-c5a2-d3b20a0d54f7"
   },
   "outputs": [],
   "source": [
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.7 * dataset_size)\n",
    "val_size   = int(0.15 * dataset_size)\n",
    "test_size  = dataset_size - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LppUg8Nd1_a"
   },
   "outputs": [],
   "source": [
    "val_dataset.dataset.transform = val_test_transform\n",
    "test_dataset.dataset.transform = val_test_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "idu4NzXId4FQ"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0YIrHOtd7NP",
    "outputId": "c88e635b-d336-4de0-fd12-b7fc439da75c"
   },
   "outputs": [],
   "source": [
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}, Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Setup & Training:**\n",
    "- We have fine-tuned a pre-trained Vision Transformer (ViT) using mixed precision to handle a larger batch size of 64 efficiently on a 15GB GPU.\n",
    "- We are using AdamW as the optimizer with a learning rate of 5e-5 and and cross-entropy as the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203,
     "referenced_widgets": [
      "2b30126c2d8246c881dcf30372daa866",
      "e1e65afda7984fb4b4de0f8e92d09bfa",
      "e38fc0a665de4076a4dbeb612a842ebe",
      "aca44949ef604d91ae11be0623f59e23",
      "cc66b8ad9a7542339cbb2be8c0e148f7",
      "3719e30df6c54f3685591fc4803c0453",
      "9915e3efb5504754bea9eb6880d112d9",
      "ab418153e346408ab4ae5b7ba28ed2ed",
      "717b7ef1676a43f09a51c7b0a752763f",
      "82f4d1ff0120432e9f843fd5592c3589",
      "fabebf8504014d849c6bd84469a5dd9f",
      "6965b7f439134102a0dd6bd0a4e24f76",
      "322d9b2bf4cf4d4b88905e4cb2d49e15",
      "79a996c09f454018bafc962a65397af7",
      "0fc71bcd17184e079384d675d90ebd1e",
      "eb1da5b066ea463198e22bd23e6e56bc",
      "3039d099786147f09b99edecd752d038",
      "6acde9b9c46b48429165e75b9d0b39a5",
      "e31b77d3dae244b69645b7ceb763012b",
      "c1f3c0a8842243b6954960c16dcfa4f1",
      "1680ad015d2749988720b7820b29cd17",
      "b691be5547b84eaaaa5ed9e4a7106ccd"
     ]
    },
    "id": "MyUwNxTAd9na",
    "outputId": "b9c07dab-e7c8-4367-e49a-c445ca92c708"
   },
   "outputs": [],
   "source": [
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\",\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vcKD6gYveD84",
    "outputId": "c8667e8e-0b0a-4de2-8a68-36bec4943cad"
   },
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    # Training Phase.\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc  = correct / total\n",
    "\n",
    "    # Validation Phase.\n",
    "    model.eval()\n",
    "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc  = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Metrics:**\n",
    "- In epoch 1 Train Loss = 0.0302 (99.05% accuracy), Val Loss = 0.0166 (99.49% accuracy).\n",
    "- But in others we received almost perfect training and validation accuracies with minor fluctuations in loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzVtSQg2eNcc",
    "outputId": "55b1c71e-4499-4e70-945d-5664da7fda38"
   },
   "outputs": [],
   "source": [
    "model_save_path = \"vit_cats_dogs.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oOKa7ylTjBCY",
    "outputId": "857c7c03-f523-4c22-faaf-8e90478d5ed0"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_loss /= test_total\n",
    "test_acc  = test_correct / test_total\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we received excellent accuracy on test set as well. So, The high accuracy in training, validation, and test sets menas that our fine-tuned ViT model is highly effective at classifying cat vs. dog images with excellent generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is app.py file that I used to deploy model on hugging face using gradio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
    "from PIL import Image\n",
    "import gradio as gr\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "])\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    \"google/vit-base-patch16-224\", \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True \n",
    ")\n",
    "model.load_state_dict(torch.load(\"vit_cats_dogs.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def classify_image(image: Image.Image):\n",
    "    \n",
    "    image = image.convert(\"RGB\")\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(img_tensor).logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "    \n",
    "    prob_cat = probs[0][0].item()\n",
    "    prob_dog = probs[0][1].item()\n",
    "    label = \"Cat\" if prob_cat > prob_dog else \"Dog\"\n",
    "    return label, {\"Cat\": prob_cat, \"Dog\": prob_dog}\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=classify_image,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Upload an image\"),\n",
    "    outputs=[\n",
    "        gr.Label(label=\"Predicted Label\"),\n",
    "        gr.JSON(label=\"Probabilities\")\n",
    "    ],\n",
    "    title=\"Cat vs. Dog Classifier\",\n",
    "    description=\"Upload an image of a cat or a dog and get the prediction.\"\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "- PyTorch: https://pytorch.org/docs/stable/index.html\n",
    "- Dataset: https://www.microsoft.com/en-us/download/details.aspx?id=54765\n",
    "- Torchvision: https://pytorch.org/vision/stable/index.html\n",
    "- Transformers: https://huggingface.co/docs/transformers\n",
    "- Gradio: https://www.gradio.app/docs\n",
    "- Pillow: https://pillow.readthedocs.io/en/stable/\n",
    "- Hugging face spaces: https://huggingface.co/docs/hub/spaces\n",
    "- ViT pretrained model: https://huggingface.co/google/vit-base-patch16-224\n",
    "- ViT paper: https://arxiv.org/pdf/2010.11929"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F13DrAjseZMc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H_skCx4hfU47",
    "outputId": "a9cad11c-035d-407a-8709-70eeba32926b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SWvHj5wpgce3",
    "outputId": "6bd57f8e-bcc7-4a0a-c01d-26932f4e4b72"
   },
   "outputs": [],
   "source": [
    "!unzip \"/content/drive/MyDrive/DL/cnn_dataset.zip\" -d \"/content/cnn_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uvmm1bjygtk0",
    "outputId": "abd4b630-bc5a-46d0-e9ae-94c507bccbe5"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "data_dir = '/content/cnn_dataset'\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "\n",
    "print(\"Number of images:\", len(dataset))\n",
    "print(\"Classes:\", dataset.classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pg4PER1pgzVk",
    "outputId": "c43c27cd-a8c7-4919-d981-3ffc6298b144"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "class_counts = Counter(label for _, label in dataset.samples)\n",
    "\n",
    "print(\"Number of images per class:\")\n",
    "for class_idx, count in class_counts.items():\n",
    "    class_name = dataset.classes[class_idx]\n",
    "    print(f\"{class_name}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa0Fmnx3hNEt"
   },
   "outputs": [],
   "source": [
    "data_indices = list(range(len(dataset)))\n",
    "train_idx, test_idx = train_test_split(data_indices, test_size=0.2, stratify=[s[1] for s in dataset.imgs], random_state=42)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.1, stratify=[dataset.imgs[i][1] for i in train_idx], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "diubriobhTYN"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "val_dataset   = Subset(dataset, val_idx)\n",
    "test_dataset  = Subset(dataset, test_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wLwBAJcF5pfi"
   },
   "source": [
    "Step 1 - Defining VGG-Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B004Bz-EhWkF"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLb3U3-jhYLh"
   },
   "outputs": [],
   "source": [
    "class VGGDeep(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_size=(3, 64, 64)):\n",
    "        super(VGGDeep, self).__init__()\n",
    "\n",
    "        # Feature extraction part (VGG-16 base + extra conv block)\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Extra Block (Block 6)\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.flattened_size = self._get_conv_output(input_size)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            output_feat = self.features(dummy_input)\n",
    "            n_size = output_feat.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8OlMH5K5v40"
   },
   "source": [
    "Step 2 - Training VGG-Deep based on restrictions and constraints provided in question to observe effect of depth in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GK-mmR13kegH",
    "outputId": "90a071c5-cc25-4945-8c6f-ffebae11be9a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_classes = 3\n",
    "model = VGGDeep(num_classes=num_classes)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)  # Simple SGD without momentum\n",
    "\n",
    "num_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_acc = []\n",
    "val_acc = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss_val / len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} || \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "model.eval()\n",
    "running_loss_test = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss_test += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_loss = running_loss_test / len(test_loader.dataset)\n",
    "test_accuracy = correct_test / total_test\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "ol3K-f8scUKB",
    "outputId": "2f9139c7-aa7b-4af5-e9fd-7ceea14c1778"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs + 1), train_acc, label='Train Accuracy', marker='o')\n",
    "plt.plot(range(1, num_epochs + 1), val_acc, label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30Rly1sR57oL"
   },
   "source": [
    "Step 3 - Gradient analysis using hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fh1ZmvGmo4gN"
   },
   "outputs": [],
   "source": [
    "class VGGDeep(nn.Module):\n",
    "    def __init__(self, num_classes=3, input_size=(3, 64, 64)):\n",
    "        super(VGGDeep, self).__init__()\n",
    "\n",
    "        # Feature extraction part (VGG-16 base + extra conv block)\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            # Extra Block (Block 6)\n",
    "            nn.Conv2d(512, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(1024, 1024, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Determine the size of the flattened features dynamically\n",
    "        self.flattened_size = self._get_conv_output(input_size)\n",
    "\n",
    "        # Classifier part\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.flattened_size, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, *shape)\n",
    "            output_feat = self.features(dummy_input)\n",
    "            n_size = output_feat.view(1, -1).size(1)\n",
    "        return n_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wvakv9TBiBHX"
   },
   "outputs": [],
   "source": [
    "gradient_data = {\n",
    "    'norms': [],\n",
    "    'layer_names': []\n",
    "}\n",
    "\n",
    "def gradient_hook(module, grad_input, grad_output):\n",
    "    \"\"\"Hook function to capture gradient norms\"\"\"\n",
    "    if grad_output[0] is not None:\n",
    "        grad_norm = grad_output[0].norm(p=2).item()\n",
    "        gradient_data['norms'][-1].append(grad_norm)\n",
    "\n",
    "# Register hooks for all convolutional layers\n",
    "conv_layers = [layer for layer in model.features if isinstance(layer, nn.Conv2d)]\n",
    "for idx, layer in enumerate(conv_layers):\n",
    "    gradient_data['layer_names'].append(f\"Conv{idx+1}\")\n",
    "    layer.register_full_backward_hook(gradient_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "1PqJrarxipAt",
    "outputId": "e57de281-c30e-4e56-da09-f7e04d105d07"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Gradient tracking setup\n",
    "gradient_data = {\n",
    "    'norms': [],\n",
    "    'layer_names': []\n",
    "}\n",
    "\n",
    "def gradient_hook(module, grad_input, grad_output):\n",
    "    \"\"\"Hook function to capture gradient norms\"\"\"\n",
    "    if grad_output[0] is not None:\n",
    "        grad_norm = grad_output[0].norm(p=2).item()\n",
    "        # Append to current batch's list\n",
    "        if gradient_data['norms'] and gradient_data['norms'][-1]:\n",
    "            gradient_data['norms'][-1][-1].append(grad_norm)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 3\n",
    "model = VGGDeep(num_classes=num_classes)\n",
    "\n",
    "# Register hooks on convolutional layers\n",
    "conv_layers = []\n",
    "for i, layer in enumerate(model.features):\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        conv_layers.append(layer)\n",
    "        gradient_data['layer_names'].append(f\"Conv{len(conv_layers)}\")\n",
    "        layer.register_full_backward_hook(gradient_hook)\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "track_interval = 10  # Track gradients every 10 batches\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Initialize epoch entry with empty list for batches\n",
    "    gradient_data['norms'].append([])\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store gradients at specified intervals\n",
    "        if batch_idx % track_interval == 0:\n",
    "            gradient_data['norms'][-1].append([])\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_loss_val = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss_val += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += predicted.eq(labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss_val / len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_losses.append(val_loss)\n",
    "    val_acc.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f} || \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Visualization code\n",
    "def plot_gradient_norms(gradient_data, selected_layers=None):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Plot all layers\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for layer_idx in range(len(gradient_data['layer_names'])):\n",
    "        layer_grads = []\n",
    "        for epoch in gradient_data['norms']:\n",
    "            for batch in epoch:\n",
    "                if len(batch) > layer_idx:\n",
    "                    layer_grads.extend([batch[layer_idx]])\n",
    "        plt.plot(layer_grads, label=gradient_data['layer_names'][layer_idx])\n",
    "\n",
    "    plt.title('Gradient Norms - All Layers')\n",
    "    plt.xlabel(f'Batches (Every {track_interval}th)')\n",
    "    plt.ylabel('L2 Norm')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot selected layers\n",
    "    if selected_layers:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        for idx in selected_layers:\n",
    "            layer_grads = []\n",
    "            for epoch in gradient_data['norms']:\n",
    "                for batch in epoch:\n",
    "                    if len(batch) > idx:\n",
    "                        layer_grads.extend([batch[idx]])\n",
    "            plt.plot(layer_grads,\n",
    "                    label=f\"{gradient_data['layer_names'][idx]} (Depth {idx+1})\")\n",
    "\n",
    "        plt.title('Gradient Norms - Depth Comparison')\n",
    "        plt.xlabel(f'Batches (Every {track_interval}th)')\n",
    "        plt.ylabel('L2 Norm')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate plots\n",
    "plot_gradient_norms(gradient_data, selected_layers=[1, 4, 7, len(conv_layers)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlCavTq26Ebr"
   },
   "source": [
    "Step 4 - Comparison with VGG-16 and Resnet-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "PuDMRy75jWkM",
    "outputId": "3517460e-62fc-4c18-8a18-1b35a3810fcb"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Epochs\n",
    "epochs = np.arange(1, 21)\n",
    "\n",
    "resnet18_train = train_acc_resnet\n",
    "resnet18_val = val_acc_resnet\n",
    "vgg16_train = train_acc_vgg\n",
    "vgg16_val = val_acc_vgg\n",
    "vggdeep_train = train_acc\n",
    "vggdeep_val = val_acc\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# ResNet-18\n",
    "plt.plot(epochs,resnet18_train,label=\"ResNet-18 Train Acc\",color=\"red\",linestyle=\"-\")\n",
    "plt.plot(epochs,resnet18_val,label=\"ResNet-18 Val Acc\",color=\"red\",linestyle=\"--\")\n",
    "\n",
    "# VGG-16\n",
    "plt.plot(epochs,vgg16_train,label=\"VGG-16 Train Acc\",color=\"blue\",linestyle=\"-\")\n",
    "plt.plot(epochs,vgg16_val,label=\"VGG-16 Val Acc\",color=\"blue\",linestyle=\"--\")\n",
    "\n",
    "# VGG-Deep\n",
    "plt.plot(epochs,vggdeep_train,label=\"VGG-Deep Train Acc\",color=\"green\",linestyle=\"-\")\n",
    "plt.plot(epochs,vggdeep_val,label=\"VGG-Deep Val Acc\",color=\"green\",linestyle=\"--\")\n",
    "\n",
    "# Labels and Legend\n",
    "plt.title(\"Training and Validation Accuracy for ResNet-18,VGG-16,and VGG-Deep\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "iQCn22D_kO8I",
    "outputId": "93e90052-2230-4e29-caec-24256074797c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Epochs\n",
    "epochs = np.arange(1, 21)\n",
    "resnet18_train_loss = train_loss_resnet\n",
    "resnet18_val_loss = val_loss_resnet\n",
    "\n",
    "vgg16_train_loss = train_loss_vgg\n",
    "vgg16_val_loss = val_loss_vgg\n",
    "\n",
    "vggdeep_train_loss = train_losses\n",
    "vggdeep_val_loss = val_losses\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# ResNet-18\n",
    "plt.plot(epochs, resnet18_train_loss, label=\"ResNet-18 Train Loss\", color=\"red\", linestyle=\"-\")\n",
    "plt.plot(epochs, resnet18_val_loss, label=\"ResNet-18 Val Loss\", color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# VGG-16\n",
    "plt.plot(epochs, vgg16_train_loss, label=\"VGG-16 Train Loss\", color=\"blue\", linestyle=\"-\")\n",
    "plt.plot(epochs, vgg16_val_loss, label=\"VGG-16 Val Loss\", color=\"blue\", linestyle=\"--\")\n",
    "\n",
    "# VGG-Deep\n",
    "plt.plot(epochs, vggdeep_train_loss, label=\"VGG-Deep Train Loss\", color=\"green\", linestyle=\"-\")\n",
    "plt.plot(epochs, vggdeep_val_loss, label=\"VGG-Deep Val Loss\", color=\"green\", linestyle=\"--\")\n",
    "\n",
    "# Labels and Legend\n",
    "plt.title(\"Training and Validation Loss for ResNet-18, VGG-16, and VGG-Deep\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5pcNH2r6OrB"
   },
   "source": [
    "Step 5 - 3 more setups on base VGG. Choosing kernel size, max and avg pooling, activation function variations(subparts a,c,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gf0sq2AelLvU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(VGG16, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 4 * 4, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "vgg_model = VGG16(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3ebMOv-m6zl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "import os  # Import the os module\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a common training function\n",
    "def train_and_validate(model, train_loader, val_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.to(device)\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = running_loss / len(val_loader.dataset)\n",
    "        val_acc = correct / total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "\n",
    "# Function to plot training and validation curves\n",
    "def plot_curves(train_losses, val_losses, train_accs, val_accs, title):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='Training Loss')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='Validation Loss')\n",
    "    plt.title('Loss Curves - {}'.format(title))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'b-', label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accs, 'r-', label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves - {}'.format(title))\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jcAr2uqEtvMu",
    "outputId": "a8e8a6c9-4dac-4a64-d6fe-56baeaeb13af"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "sample_image_path = dataset.imgs[0][0]\n",
    "sample_image = Image.open(sample_image_path)\n",
    "image_width, image_height = sample_image.size\n",
    "print(f\"Image width: {image_width}, height: {image_height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIZxiIByqr8Q"
   },
   "outputs": [],
   "source": [
    "class SmallVGG(nn.Module):\n",
    "    def __init__(self, kernel_size=3, num_classes=len(dataset.classes)):\n",
    "        super(SmallVGG, self).__init__()\n",
    "        # Use kernel_size//2 for padding to maintain spatial dimensions\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=kernel_size, padding=kernel_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Calculate feature map size with a dummy input\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, image_height, image_width)\n",
    "            output = self.features(dummy_input)\n",
    "            flattened_size = output.numel() // output.size(0)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        print(f\"Kernel size: {kernel_size}, Feature map output shape: {output.shape}, Flattened size: {flattened_size}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 2. Max Pooling vs. Average Pooling - FIXED VERSION\n",
    "class SmallVGGPooling(nn.Module):\n",
    "    def __init__(self, pooling_type='max', num_classes=len(dataset.classes)):\n",
    "        super(SmallVGGPooling, self).__init__()\n",
    "        if pooling_type == 'max':\n",
    "            pool = nn.MaxPool2d\n",
    "        elif pooling_type == 'avg':\n",
    "            pool = nn.AvgPool2d\n",
    "        else:\n",
    "            raise ValueError(\"pooling_type must be 'max' or 'avg'\")\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            pool(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            pool(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            pool(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Calculate feature map size with a dummy input\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, image_height, image_width)\n",
    "            output = self.features(dummy_input)\n",
    "            flattened_size = output.numel() // output.size(0)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        print(f\"Pooling type: {pooling_type}, Feature map output shape: {output.shape}, Flattened size: {flattened_size}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# 3. Activation Functions - FIXED VERSION\n",
    "class SmallVGGActivation(nn.Module):\n",
    "    def __init__(self, activation_type='relu', num_classes=len(dataset.classes)):\n",
    "        super(SmallVGGActivation, self).__init__()\n",
    "        if activation_type == 'relu':\n",
    "            activation = nn.ReLU(inplace=True)\n",
    "        elif activation_type == 'leaky_relu':\n",
    "            activation = nn.LeakyReLU(inplace=True)\n",
    "        elif activation_type == 'elu':\n",
    "            activation = nn.ELU(inplace=True)\n",
    "        elif activation_type == 'gelu':\n",
    "            activation = nn.GELU()\n",
    "        else:\n",
    "            raise ValueError(\"activation_type must be 'relu', 'leaky_relu', 'elu', or 'gelu'\")\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            activation,\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, image_height, image_width)\n",
    "            output = self.features(dummy_input)\n",
    "            flattened_size = output.numel() // output.size(0)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(flattened_size, 128),\n",
    "            activation,\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        print(f\"Activation type: {activation_type}, Feature map output shape: {output.shape}, Flattened size: {flattened_size}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "q9Vr6-BEq11n",
    "outputId": "86754f62-ad2c-4a5c-9cfc-21439fb9f8db"
   },
   "outputs": [],
   "source": [
    "kernel_sizes = [3, 5, 7]\n",
    "kernel_results = defaultdict(dict)\n",
    "\n",
    "for k in kernel_sizes:\n",
    "    print(f\"Training SmallVGG with kernel size: {k}\")\n",
    "    model = SmallVGG(kernel_size=k)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_losses, train_accs, val_accs = train_and_validate(\n",
    "        model, train_loader, val_loader, optimizer, criterion, num_epochs=5\n",
    "    )\n",
    "    kernel_results[k]['train_losses'] = train_losses\n",
    "    kernel_results[k]['val_losses'] = val_losses\n",
    "    kernel_results[k]['train_accs'] = train_accs\n",
    "    kernel_results[k]['val_accs'] = val_accs\n",
    "    plot_curves(train_losses, val_losses, train_accs, val_accs, f\"Kernel Size {k}\")\n",
    "\n",
    "pooling_types = ['max', 'avg']\n",
    "pooling_results = defaultdict(dict)\n",
    "\n",
    "for pool_type in pooling_types:\n",
    "    print(f\"Training SmallVGG with {pool_type} pooling\")\n",
    "    model = SmallVGGPooling(pooling_type=pool_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_losses, train_accs, val_accs = train_and_validate(\n",
    "        model, train_loader, val_loader, optimizer, criterion, num_epochs=5\n",
    "    )\n",
    "    pooling_results[pool_type]['train_losses'] = train_losses\n",
    "    pooling_results[pool_type]['val_losses'] = val_losses\n",
    "    pooling_results[pool_type]['train_accs'] = train_accs\n",
    "    pooling_results[pool_type]['val_accs'] = val_accs\n",
    "    plot_curves(train_losses, val_losses, train_accs, val_accs, f\"{pool_type} Pooling\")\n",
    "\n",
    "activation_types = ['relu', 'leaky_relu', 'elu', 'gelu']\n",
    "activation_results = defaultdict(dict)\n",
    "\n",
    "for act_type in activation_types:\n",
    "    print(f\"Training SmallVGG with {act_type} activation\")\n",
    "    model = SmallVGGActivation(activation_type=act_type)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses, val_losses, train_accs, val_accs = train_and_validate(\n",
    "        model, train_loader, val_loader, optimizer, criterion, num_epochs=5\n",
    "    )\n",
    "    activation_results[act_type]['train_losses'] = train_losses\n",
    "    activation_results[act_type]['val_losses'] = val_losses\n",
    "    activation_results[act_type]['train_accs'] = train_accs\n",
    "    activation_results[act_type]['val_accs'] = val_accs\n",
    "    plot_curves(train_losses, val_losses, train_accs, val_accs, f\"{act_type} Activation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odL_BssWzB59"
   },
   "source": [
    "a. The gradient norm plots demonstrate the vanishing gradient problem. As we move deeper into the network, the gradient norm changes substantially. The gradient norm of layer 1 is 3.5x larger than that of layer 2.\n",
    "\n",
    "b. The vanishing gradient problem happens in deep neural networks during the backprop phase. We now that during backprop, there is an effect of chain rule of derivates that take place and since the values normally lie between -1 and 1, this multiplication chain leads to values being so small that there is no learning.\n",
    "\n",
    "c. ResNet introduces residual connections that skip one or more layers. Instead of learning a direct mapping from input to output, the network learns the residual, i.e., the difference between the input and the output. This is achieved by adding the input of a layer to its output, creating a shortcut connection.\n",
    "The identity mapping x ensures that the gradient can flow directly through the shortcut connection without being multiplied by small weights. This helps in preserving the gradient magnitude, making it easier for the network to propagate gradients back to the earlier layers.\n",
    "\n",
    "d.Batch normalization, introduced by Ioffe and Szegedy in 2015, helps stabilize and accelerate the training of deep neural networks by normalizing the inputs of each layer.\n",
    "\n",
    "Impact on Vanishing/Exploding Gradient Problem\n",
    "Batch normalization addresses the vanishing/exploding gradient problem by normalizing the activations of each layer to have zero mean and unit variance. This normalization helps in several ways:\n",
    "\n",
    "-Stabilizes Gradient Flow: By normalizing the inputs to each layer, batch normalization prevents the activations from becoming too large or too small, which helps in maintaining a stable gradient flow throughout the network.\n",
    "\n",
    "-Reduces Internal Covariate Shift: Internal covariate shift refers to the change in the distribution of network activations due to the continual change in network parameters during training. Batch normalization reduces this shift, making the training process more stable.\n",
    "\n",
    "-Allows Higher Learning Rates: With batch normalization, the network can use higher learning rates without the risk of gradients exploding. This accelerates the training process.\n",
    "\n",
    "-Regularization Effect: Batch normalization also has a slight regularizing effect, similar to dropout, which can help in reducing overfitting.\n",
    "\n",
    "e.\n",
    "Key findings from my three chosen investigataions are\n",
    "\n",
    "-Kernel sizes 3 and 5 have better performance than kernel size 7.\n",
    "-Max pooling has a better performance than average pooling\n",
    "-ReLU and leaky ReLu have almost similar performances but are much better than ELU and GELU activation functions. The performance gap is around 4-5%.\n",
    "\n",
    "f. References\n",
    "\n",
    "- https://pytorch.org/vision/main/models/generated/torchvision.models.vgg16.html\n",
    "- https://www.v7labs.com/blog/neural-networks-activation-functions\n",
    "- https://pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_full_backward_hook.html\n",
    "- https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
